{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T13:59:03.165605Z",
     "start_time": "2020-05-21T13:59:03.161283Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_dataset(file):\n",
    "    load_file = open(file, \"rb\")\n",
    "    dataset = pickle.load(load_file)\n",
    "    load_file.close()\n",
    "    return dataset\n",
    "\n",
    "def dump_dataset(dataset, file):\n",
    "    file = open(file, \"wb\")\n",
    "    pickle.dump(dataset, file)\n",
    "    file.close()\n",
    "\n",
    "def dump_features(feature_array, file):\n",
    "    write_file = open(file, \"wb\")\n",
    "    pickle.dump(feature_array, write_file)\n",
    "    write_file.close()\n",
    "\n",
    "def load_features(file):\n",
    "    load_file = open(file, \"rb\")\n",
    "    features = pickle.load(load_file)\n",
    "    load_file.close()\n",
    "    return features\n",
    "\n",
    "def dump_labels(labels, file):\n",
    "    write_file = open(file, \"wb\")\n",
    "    pickle.dump(labels, write_file)\n",
    "    write_file.close()\n",
    "\n",
    "\n",
    "def load_labels(file):\n",
    "    load_file = open(file, \"rb\")\n",
    "    labels = pickle.load(load_file)\n",
    "    load_file.close()\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T13:59:11.623218Z",
     "start_time": "2020-05-21T13:59:05.475704Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "from hooks.pretokenization import *\n",
    "from hooks.posttokenization import *\n",
    "from hooks.spell_check import *\n",
    "from hooks.annotation_normalization import *\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg', disable=[\"parser\", \"ner\"])\n",
    "\n",
    "def tokenize(raw, tokenizer=\"split\"):\n",
    "    if tokenizer == \"spacy\":\n",
    "        return [token.text for token in nlp.tokenizer(raw)]\n",
    "    if tokenizer == \"split\":\n",
    "        return raw.split(\" \")\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    lem = list()\n",
    "    for token in tokens:\n",
    "        lem.append(lemmatizer.lemmatize(token))\n",
    "    return lem\n",
    "\n",
    "\n",
    "def build_vocab(data):\n",
    "    vocab = dict()\n",
    "    index = 1\n",
    "    for sent in data:\n",
    "        for word in sent:\n",
    "            if word not in vocab.keys():\n",
    "                vocab.update({word: index})\n",
    "                index += 1\n",
    "    return vocab\n",
    "\n",
    "def encode_sentence(sentence, vocab):\n",
    "    encoded = list()\n",
    "    for word in sentence:\n",
    "        if word in vocab.keys():\n",
    "            encoded.append(vocab[word])\n",
    "        else:\n",
    "            encoded.append(0)\n",
    "    return encoded\n",
    "\n",
    "\n",
    "def encode_data(data, vocab):\n",
    "    encoded_data = list()\n",
    "    for sent in data:\n",
    "        encoded_data.append(encode_sentence(sent, vocab))\n",
    "    return encoded_data\n",
    "\n",
    "\n",
    "def split_train_validate_test(data, labels, train_valtest_ratio, validate_test_ratio, random_state=42):\n",
    "    X_train, X_valtest, y_train, y_valtest = train_test_split(data, labels, test_size=train_valtest_ratio,\n",
    "                                                              random_state=random_state)\n",
    "    X_validate, X_test, y_validate, y_test = train_test_split(X_valtest, y_valtest, test_size=validate_test_ratio,\n",
    "                                                              random_state=random_state)\n",
    "\n",
    "    return X_train, X_validate, X_test, y_train, y_validate, y_test\n",
    "\n",
    "def process_dataset(data):\n",
    "    dataset = list()\n",
    "    for i in tqdm(range(len(data))):\n",
    "        new_tweet = repair_chars(data[i])\n",
    "        # anot = copy.deepcopy(new_tweet)\n",
    "        anot = new_tweet\n",
    "        # new_tweet = remove_usernames(new_tweet)\n",
    "        # new_tweet = remove_links(new_tweet)\n",
    "        # new_tweet = remove_punctuation(new_tweet)\n",
    "        #\n",
    "        # tweet_tokens = tokenize(new_tweet, tokenizer=\"spacy\")\n",
    "        # tweet_tokens = remove_stopwords(raw=\"\", tokenized=tweet_tokens)\n",
    "\n",
    "        anot = annotation_normalization(anot)\n",
    "        anot_tokens = tokenize(anot, tokenizer=\"split\")\n",
    "        anot_tokens = spell_check_tokens(anot_tokens)\n",
    "        anot_tokens = replace_slang_tokens(anot_tokens)\n",
    "        anot_tokens = remove_stopwords_tokens(anot_tokens)\n",
    "        # anot_tokens = lemmatize(anot_tokens)\n",
    "\n",
    "        # dataset.append({\"tweet\": new_tweet, \"tweet_tokens\": tweet_tokens, \"anot\": anot, \"anot_tokens\": anot_tokens})\n",
    "        dataset.append({\"anot_tokens\": anot_tokens})\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def create_vocab_encode_data(tokens):\n",
    "    vocab = build_vocab(tokens)\n",
    "    encoded_data = encode_data(data=tokens, vocab=vocab)\n",
    "    return vocab, encoded_data\n",
    "\n",
    "\n",
    "def pad_encoded_data(encoded, seq_length):\n",
    "    features = np.zeros((len(encoded), seq_length), dtype=int)\n",
    "    for i, review in enumerate(encoded):\n",
    "        if len(review) > seq_length:\n",
    "            review = review[:seq_length]\n",
    "        zeroes = list(np.zeros(seq_length - len(review)))\n",
    "        new = zeroes + review\n",
    "        features[i, :] = np.array(new)\n",
    "    return features\n",
    "\n",
    "\n",
    "def make_embedding_matrix(vocab, embedding_dim=300):\n",
    "    hits, misses = 0, 0\n",
    "    embedding_matrix = np.zeros((len(vocab) + 1, embedding_dim))\n",
    "    for word, i in vocab.items():\n",
    "        token = nlp(word)\n",
    "        if token.has_vector:\n",
    "            embedding_matrix[i] = token.vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "\n",
    "    print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T10:21:06.467612Z",
     "start_time": "2020-05-21T10:21:06.466237Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T14:00:45.458003Z",
     "start_time": "2020-05-21T14:00:45.449657Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Masking, Embedding\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def compile_model(vocab, embedding_matrix, input_length,\n",
    "                  trainable=False,\n",
    "                  recurrent_layer_size=256,\n",
    "                  dense_size=256,\n",
    "                  dropout=0.1,\n",
    "                  recurrent_dropout=0.1,\n",
    "                  dense_activation='relu',\n",
    "                  dropout_for_regularization=0.5,\n",
    "                  output_activation='softmax',\n",
    "                  optimizer='adam',\n",
    "                  metrics = tf.keras.metrics.Recall(),\n",
    "                  loss=tf.keras.losses.SquaredHinge(reduction=\"auto\", name=\"squared_hinge\")):\n",
    "\n",
    "    model = Sequential()\n",
    "    # Embedding layer\n",
    "    model.add(\n",
    "        Embedding(input_dim=len(vocab) + 1,\n",
    "                  input_length=input_length,\n",
    "                  output_dim=300,\n",
    "                  weights=[embedding_matrix],\n",
    "                  trainable=False,\n",
    "                  mask_zero=True))\n",
    "\n",
    "    # Masking layer for pre-trained embeddings\n",
    "    model.add(Masking(mask_value=0.0))\n",
    "\n",
    "    # Recurrent layer\n",
    "    model.add(LSTM(recurrent_layer_size, return_sequences=False, dropout=dropout, recurrent_dropout=recurrent_dropout,\n",
    "                   input_shape=(2048, 28, 300)))\n",
    "    # model.add(LSTM(recurrent_layer_size, return_sequences=False, dropout=dropout, recurrent_dropout=recurrent_dropout))\n",
    "\n",
    "    # # Second layer (batch_size, sequence_length, features)\n",
    "    # model.add(LSTM(int(recurrent_layer_size / 4), return_sequences=True))\n",
    "    # model.add(LSTM(int(recurrent_layer_size / 8), return_sequences=False))\n",
    "\n",
    "    # Fully connected layer\n",
    "    model.add(Dense(dense_size, activation=dense_activation))\n",
    "\n",
    "    # Dropout for regularization\n",
    "    model.add(Dropout(dropout_for_regularization))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(3, activation=output_activation))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=[metrics]\n",
    "        )\n",
    "    return model\n",
    "\n",
    "\n",
    "# CHANGE PATH FOR SERVER\n",
    "local = \"/home/ivan/Documents/git_repos/Sentiment-Analysis-on-Twitter/models/model.h5\"\n",
    "djurdja = '/home/ikrizanic/pycharm/zavrsni/models/model.h5'\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=20),\n",
    "             ModelCheckpoint(local)]\n",
    "\n",
    "\n",
    "def fit_model(model, X_train, y_train, X_val, y_val, batch_size=2048, epochs = 10):\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        batch_size=batch_size, epochs=epochs,\n",
    "                        callbacks=callbacks,\n",
    "                        validation_data=(X_val, y_val))\n",
    "    return history, model\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    res = model.evaluate(X_test, y_test)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T13:59:15.096541Z",
     "start_time": "2020-05-21T13:59:14.862868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading pickle data...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "local_path = \"/home/ivan/Documents/git_repos/Sentiment-Analysis-on-Twitter/data\"\n",
    "djurdja_path = \"/home/ikrizanic/pycharm/zavrsni/data\"\n",
    "working_path = local_path\n",
    "data_paths = {\"train_dataset\": working_path + \"/lstm/train_dataset.pl\",\n",
    "              \"input_dataset\": working_path + \"/lstm/input_dataset.pl\",\n",
    "              \"input_labels\": working_path + \"/lstm/input_labels.pl\",\n",
    "              \"test_dataset\": working_path + \"/lstm/test_dataset.pl\",\n",
    "              \"train_labels\": working_path + \"/lstm/train_labels.pl\",\n",
    "              \"test_labels\": working_path + \"/lstm/test_labels.pl\",\n",
    "              \"embedding_matrix\": working_path + \"/lstm/embedding_matrix.pl\"}\n",
    "\n",
    "dataset_name = \"main\" + \"_data\"\n",
    "djurdja_paths = {\"dataset\": str(\"~/pycharm/zavrsni/data/\" + dataset_name + \".csv\"),\n",
    "                 \"labels\": \"/home/ikrizanic/pycharm/zavrsni/data/labels.txt\"}\n",
    "local_paths = {\n",
    "    \"dataset\": str(\"/home/ivan/Documents/git_repos/Sentiment-Analysis-on-Twitter/data/\" + dataset_name + \".csv\"),\n",
    "    \"labels\": \"/home/ivan/Documents/git_repos/Sentiment-Analysis-on-Twitter/data/labels.txt\"}\n",
    "\n",
    "train_dataset = pd.read_csv(local_paths[\"dataset\"], sep=\"\\t\", names=[\"label\", \"text\"])\n",
    "# train_dataset = pd.read_csv(\n",
    "# '/home/ivan/Documents/git_repos/Sentiment-Analysis-on-Twitter/data/SemEval2017-task4-dev.subtask-A.english.INPUT.txt',\n",
    "#                             sep=\"\\t\", quotechar='\"',\n",
    "#                             names=[\"id\", \"label\", \"text\"])\n",
    "\n",
    "test_dataset = pd.read_csv(\n",
    "    '/home/ivan/Documents/git_repos/Sentiment-Analysis-on-Twitter/data/SemEval2017-task4-test.subtask-A.english.txt',\n",
    "    sep=\"\\t\", quotechar='\\'', names=[\"id\", \"label\", \"text\"])\n",
    "#\n",
    "# train_dataset, train_labels = return_tweets_and_labels(train_dataset)\n",
    "# test_dataset, test_labels = return_tweets_and_labels(test_dataset)\n",
    "#\n",
    "# train_dataset = process_dataset(train_dataset)\n",
    "# test_dataset = process_dataset(test_dataset)\n",
    "#\n",
    "# dump_dataset(train_dataset, data_paths[\"train_dataset\"])\n",
    "# dump_dataset(test_dataset, data_paths[\"test_dataset\"])\n",
    "# dump_labels(train_labels, data_paths[\"train_labels\"])\n",
    "# dump_labels(test_labels, data_paths[\"test_labels\"])\n",
    "\n",
    "print(\"Reading pickle data...\")\n",
    "train_dataset = load_dataset(data_paths[\"train_dataset\"])\n",
    "test_dataset = load_dataset(data_paths[\"test_dataset\"])\n",
    "train_labels = load_labels(data_paths[\"train_labels\"])\n",
    "test_labels = load_labels(data_paths[\"test_labels\"])\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T13:59:17.653378Z",
     "start_time": "2020-05-21T13:59:17.077129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train vocab and data encoding...\n",
      "Done\n",
      "Padding features...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Train vocab and data encoding...\")\n",
    "train_vocab, enc_train_data = create_vocab_encode_data([d[\"anot_tokens\"] for d in train_dataset])\n",
    "enc_test_data = encode_data([d[\"anot_tokens\"] for d in test_dataset], train_vocab)\n",
    "print(\"Done\")\n",
    "\n",
    "print(\"Padding features...\")\n",
    "train_features = pad_encoded_data(enc_train_data, max(x for x in [len(d) for d in enc_train_data]))\n",
    "test_features = pad_encoded_data(enc_test_data, max(x for x in [len(d) for d in enc_train_data]))\n",
    "print(\"Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ebedding matrix...\")\n",
    "embedding_matrix = make_embedding_matrix(train_vocab)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T13:59:19.650749Z",
     "start_time": "2020-05-21T13:59:19.634870Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_features, train_labels, test_size=0.2, random_state=25, shuffle = True)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "max_len = max(x for x in [len(d) for d in enc_train_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T15:30:14.097034Z",
     "start_time": "2020-05-20T15:30:14.092357Z"
    }
   },
   "source": [
    "## Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T14:45:44.541843Z",
     "start_time": "2020-05-21T14:00:48.794301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31685 samples, validate on 7922 samples\n",
      "Epoch 1/100\n",
      "31685/31685 [==============================] - 113s 4ms/step - loss: 0.9017 - recall_2: 0.2655 - val_loss: 0.7942 - val_recall_2: 0.4161\n",
      "Epoch 2/100\n",
      "31685/31685 [==============================] - 112s 4ms/step - loss: 0.7724 - recall_2: 0.4597 - val_loss: 0.7633 - val_recall_2: 0.4896\n",
      "Epoch 3/100\n",
      "31685/31685 [==============================] - 112s 4ms/step - loss: 0.7371 - recall_2: 0.5089 - val_loss: 0.7471 - val_recall_2: 0.5237\n",
      "Epoch 4/100\n",
      "31685/31685 [==============================] - 111s 4ms/step - loss: 0.7099 - recall_2: 0.5357 - val_loss: 0.7354 - val_recall_2: 0.5461\n",
      "Epoch 5/100\n",
      "31685/31685 [==============================] - 112s 4ms/step - loss: 0.6876 - recall_2: 0.5558 - val_loss: 0.7408 - val_recall_2: 0.5627\n",
      "Epoch 6/100\n",
      "31685/31685 [==============================] - 111s 3ms/step - loss: 0.6770 - recall_2: 0.5695 - val_loss: 0.7359 - val_recall_2: 0.5758\n",
      "Epoch 7/100\n",
      "31685/31685 [==============================] - 115s 4ms/step - loss: 0.6531 - recall_2: 0.5818 - val_loss: 0.7430 - val_recall_2: 0.5865\n",
      "Epoch 8/100\n",
      "31685/31685 [==============================] - 113s 4ms/step - loss: 0.6289 - recall_2: 0.5919 - val_loss: 0.7398 - val_recall_2: 0.5965\n",
      "Epoch 9/100\n",
      "31685/31685 [==============================] - 111s 4ms/step - loss: 0.5941 - recall_2: 0.6020 - val_loss: 0.7401 - val_recall_2: 0.6063\n",
      "Epoch 10/100\n",
      "31685/31685 [==============================] - 112s 4ms/step - loss: 0.5640 - recall_2: 0.6117 - val_loss: 0.7586 - val_recall_2: 0.6158\n",
      "Epoch 11/100\n",
      "31685/31685 [==============================] - 112s 4ms/step - loss: 0.5190 - recall_2: 0.6213 - val_loss: 0.7852 - val_recall_2: 0.6256\n",
      "Epoch 12/100\n",
      "31685/31685 [==============================] - 111s 4ms/step - loss: 0.4691 - recall_2: 0.6311 - val_loss: 0.8419 - val_recall_2: 0.6355\n",
      "Epoch 13/100\n",
      "31685/31685 [==============================] - 112s 4ms/step - loss: 0.4159 - recall_2: 0.6415 - val_loss: 0.9156 - val_recall_2: 0.6462\n",
      "Epoch 14/100\n",
      "31685/31685 [==============================] - 112s 4ms/step - loss: 0.3499 - recall_2: 0.6525 - val_loss: 0.9953 - val_recall_2: 0.6574\n",
      "Epoch 15/100\n",
      "31685/31685 [==============================] - 112s 4ms/step - loss: 0.2887 - recall_2: 0.6638 - val_loss: 1.0789 - val_recall_2: 0.6688\n",
      "Epoch 16/100\n",
      "31685/31685 [==============================] - 113s 4ms/step - loss: 0.2226 - recall_2: 0.6754 - val_loss: 1.2775 - val_recall_2: 0.6805\n",
      "Epoch 17/100\n",
      "31685/31685 [==============================] - 114s 4ms/step - loss: 0.1789 - recall_2: 0.6869 - val_loss: 1.3597 - val_recall_2: 0.6918\n",
      "Epoch 18/100\n",
      "31685/31685 [==============================] - 112s 4ms/step - loss: 0.1332 - recall_2: 0.6981 - val_loss: 1.4216 - val_recall_2: 0.7027\n",
      "Epoch 19/100\n",
      "31685/31685 [==============================] - 111s 4ms/step - loss: 0.0968 - recall_2: 0.7086 - val_loss: 1.6359 - val_recall_2: 0.7131\n",
      "Epoch 20/100\n",
      "31685/31685 [==============================] - 112s 4ms/step - loss: 0.0733 - recall_2: 0.7188 - val_loss: 1.7718 - val_recall_2: 0.7229\n",
      "Epoch 21/100\n",
      "31685/31685 [==============================] - 112s 4ms/step - loss: 0.0585 - recall_2: 0.7281 - val_loss: 1.9162 - val_recall_2: 0.7319\n",
      "Epoch 22/100\n",
      "31685/31685 [==============================] - 112s 4ms/step - loss: 0.0490 - recall_2: 0.7367 - val_loss: 1.9951 - val_recall_2: 0.7403\n",
      "Epoch 23/100\n",
      "31685/31685 [==============================] - 110s 3ms/step - loss: 0.0391 - recall_2: 0.7448 - val_loss: 2.0559 - val_recall_2: 0.7481\n",
      "Epoch 24/100\n",
      "31685/31685 [==============================] - 113s 4ms/step - loss: 0.0297 - recall_2: 0.7523 - val_loss: 2.1950 - val_recall_2: 0.7553\n"
     ]
    }
   ],
   "source": [
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "sq_hinge = tf.keras.losses.SquaredHinge(reduction=\"auto\", name=\"squared_hinge\")\n",
    "model = compile_model(train_vocab, embedding_matrix, max_len,\n",
    "                      recurrent_layer_size=1024,\n",
    "                      dense_size=1024,\n",
    "                      dropout=0,\n",
    "                      recurrent_dropout=0,\n",
    "                      dense_activation='relu',\n",
    "                      dropout_for_regularization=0,\n",
    "                      output_activation='softmax',\n",
    "                      optimizer='Adam',\n",
    "                      loss=tf.keras.losses.CategoricalCrossentropy()\n",
    "                      )\n",
    "\n",
    "history, model = fit_model(model, X_train, y_train, X_val, y_val, batch_size=2048, epochs=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T14:49:13.705446Z",
     "start_time": "2020-05-21T14:48:34.502635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12258/12258 [==============================] - 39s 3ms/step\n",
      "[2.250463369056439, 0.7539452910423279]\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate(test_features, test_labels)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T10:30:22.920329Z",
     "start_time": "2020-05-21T10:30:22.917814Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import metrics\n",
    "\n",
    "def custom_metric(y_true, y_pred):\n",
    "    score = sklearn.metrics.recall_score(y_pred, y_pred, average=None)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T10:30:25.407018Z",
     "start_time": "2020-05-21T10:30:25.384574Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T14:49:50.029427Z",
     "start_time": "2020-05-21T14:49:14.039711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5931558935361216\n",
      "0.6153197233001518\n",
      "0.5676084762865792\n",
      "59.20280310409509\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = model.predict(test_features)\n",
    "\n",
    "\n",
    "c = np.eye(3, dtype=float)[np.argmax(predictions, axis=1)]\n",
    "\n",
    "tp, tn, tu, np, nn, nu = 0, 0, 0, 0, 0, 0\n",
    "for i in range(len(predictions)):\n",
    "    p = list(c[i])\n",
    "    l = list(test_labels[i])\n",
    "\n",
    "    if l == [1.0,0.0,0.0]:\n",
    "        nn += 1\n",
    "        if l == p:\n",
    "            tn += 1\n",
    "    if l == [0.0,1.0,0.0]:\n",
    "        nu += 1\n",
    "        if l == p:\n",
    "            tu += 1\n",
    "    if l == [0.0,0.0,1.0]:\n",
    "        np += 1\n",
    "        if l == p:\n",
    "            tp += 1\n",
    "    \n",
    "rp = tp/np\n",
    "ru = tu/nu\n",
    "rn = tn/nn\n",
    "print(rp)\n",
    "print(ru)\n",
    "print(rn)\n",
    "print((rp + rn + ru) * 100 / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:40:13.048170Z",
     "start_time": "2020-05-20T20:40:13.044818Z"
    }
   },
   "outputs": [],
   "source": [
    "print(tn)\n",
    "print(tu)\n",
    "print(tp)\n",
    "print(nn)\n",
    "print(nu)\n",
    "print(np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:18:41.568492Z",
     "start_time": "2020-05-20T20:18:41.566265Z"
    }
   },
   "outputs": [],
   "source": [
    "print(good)\n",
    "print(bad)\n",
    "print(good / (good + bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
