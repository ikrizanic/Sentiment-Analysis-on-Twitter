{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T09:12:43.872502Z",
     "start_time": "2020-05-21T09:12:43.868099Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_dataset(file):\n",
    "    load_file = open(file, \"rb\")\n",
    "    dataset = pickle.load(load_file)\n",
    "    load_file.close()\n",
    "    return dataset\n",
    "\n",
    "def dump_dataset(dataset, file):\n",
    "    file = open(file, \"wb\")\n",
    "    pickle.dump(dataset, file)\n",
    "    file.close()\n",
    "\n",
    "def dump_features(feature_array, file):\n",
    "    write_file = open(file, \"wb\")\n",
    "    pickle.dump(feature_array, write_file)\n",
    "    write_file.close()\n",
    "\n",
    "def load_features(file):\n",
    "    load_file = open(file, \"rb\")\n",
    "    features = pickle.load(load_file)\n",
    "    load_file.close()\n",
    "    return features\n",
    "\n",
    "def dump_labels(labels, file):\n",
    "    write_file = open(file, \"wb\")\n",
    "    pickle.dump(labels, write_file)\n",
    "    write_file.close()\n",
    "\n",
    "\n",
    "def load_labels(file):\n",
    "    load_file = open(file, \"rb\")\n",
    "    labels = pickle.load(load_file)\n",
    "    load_file.close()\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T09:12:56.273623Z",
     "start_time": "2020-05-21T09:12:45.588234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/ivan/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "from hooks.pretokenization import *\n",
    "from hooks.posttokenization import *\n",
    "from hooks.spell_check import *\n",
    "from hooks.annotation_normalization import *\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg', disable=[\"parser\", \"ner\"])\n",
    "\n",
    "def tokenize(raw, tokenizer=\"split\"):\n",
    "    if tokenizer == \"spacy\":\n",
    "        return [token.text for token in nlp.tokenizer(raw)]\n",
    "    if tokenizer == \"split\":\n",
    "        return raw.split(\" \")\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    lem = list()\n",
    "    for token in tokens:\n",
    "        lem.append(lemmatizer.lemmatize(token))\n",
    "    return lem\n",
    "\n",
    "\n",
    "def build_vocab(data):\n",
    "    vocab = dict()\n",
    "    index = 1\n",
    "    for sent in data:\n",
    "        for word in sent:\n",
    "            if word not in vocab.keys():\n",
    "                vocab.update({word: index})\n",
    "                index += 1\n",
    "    return vocab\n",
    "\n",
    "def encode_sentence(sentence, vocab):\n",
    "    encoded = list()\n",
    "    for word in sentence:\n",
    "        if word in vocab.keys():\n",
    "            encoded.append(vocab[word])\n",
    "        else:\n",
    "            encoded.append(0)\n",
    "    return encoded\n",
    "\n",
    "\n",
    "def encode_data(data, vocab):\n",
    "    encoded_data = list()\n",
    "    for sent in data:\n",
    "        encoded_data.append(encode_sentence(sent, vocab))\n",
    "    return encoded_data\n",
    "\n",
    "\n",
    "def split_train_validate_test(data, labels, train_valtest_ratio, validate_test_ratio, random_state=42):\n",
    "    X_train, X_valtest, y_train, y_valtest = train_test_split(data, labels, test_size=train_valtest_ratio,\n",
    "                                                              random_state=random_state)\n",
    "    X_validate, X_test, y_validate, y_test = train_test_split(X_valtest, y_valtest, test_size=validate_test_ratio,\n",
    "                                                              random_state=random_state)\n",
    "\n",
    "    return X_train, X_validate, X_test, y_train, y_validate, y_test\n",
    "\n",
    "def process_dataset(data):\n",
    "    dataset = list()\n",
    "    for i in tqdm(range(len(data))):\n",
    "        new_tweet = repair_chars(data[i])\n",
    "        # anot = copy.deepcopy(new_tweet)\n",
    "        anot = new_tweet\n",
    "        # new_tweet = remove_usernames(new_tweet)\n",
    "        # new_tweet = remove_links(new_tweet)\n",
    "        # new_tweet = remove_punctuation(new_tweet)\n",
    "        #\n",
    "        # tweet_tokens = tokenize(new_tweet, tokenizer=\"spacy\")\n",
    "        # tweet_tokens = remove_stopwords(raw=\"\", tokenized=tweet_tokens)\n",
    "\n",
    "        anot = annotation_normalization(anot)\n",
    "        anot_tokens = tokenize(anot, tokenizer=\"split\")\n",
    "        anot_tokens = spell_check_tokens(anot_tokens)\n",
    "        anot_tokens = replace_slang_tokens(anot_tokens)\n",
    "        anot_tokens = remove_stopwords_tokens(anot_tokens)\n",
    "        # anot_tokens = lemmatize(anot_tokens)\n",
    "\n",
    "        # dataset.append({\"tweet\": new_tweet, \"tweet_tokens\": tweet_tokens, \"anot\": anot, \"anot_tokens\": anot_tokens})\n",
    "        dataset.append({\"anot_tokens\": anot_tokens})\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def create_vocab_encode_data(tokens):\n",
    "    vocab = build_vocab(tokens)\n",
    "    encoded_data = encode_data(data=tokens, vocab=vocab)\n",
    "    return vocab, encoded_data\n",
    "\n",
    "\n",
    "def pad_encoded_data(encoded, seq_length):\n",
    "    features = np.zeros((len(encoded), seq_length), dtype=int)\n",
    "    for i, review in enumerate(encoded):\n",
    "        if len(review) > seq_length:\n",
    "            review = review[:seq_length]\n",
    "        zeroes = list(np.zeros(seq_length - len(review)))\n",
    "        new = zeroes + review\n",
    "        features[i, :] = np.array(new)\n",
    "    return features\n",
    "\n",
    "\n",
    "def make_embedding_matrix(vocab, embedding_dim=300):\n",
    "    hits, misses = 0, 0\n",
    "    embedding_matrix = np.zeros((len(vocab) + 1, embedding_dim))\n",
    "    for word, i in vocab.items():\n",
    "        token = nlp(word)\n",
    "        if token.has_vector:\n",
    "            embedding_matrix[i] = token.vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "\n",
    "    print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T09:23:13.864627Z",
     "start_time": "2020-05-21T09:23:13.862393Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import metrics\n",
    "def result_fun(y_true, y_pred):\n",
    "    score = sklearn.metrics.recall_score(y_pred, y_pred, average=None)\n",
    "    return np.array(score).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T09:30:26.456703Z",
     "start_time": "2020-05-21T09:30:26.448214Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Masking, Embedding\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def compile_model(vocab, embedding_matrix, input_length,\n",
    "                  trainable=False,\n",
    "                  recurrent_layer_size=256,\n",
    "                  dense_size=256,\n",
    "                  dropout=0.1,\n",
    "                  recurrent_dropout=0.1,\n",
    "                  dense_activation='relu',\n",
    "                  dropout_for_regularization=0.5,\n",
    "                  output_activation='softmax',\n",
    "                  optimizer='adam',\n",
    "                  metrics = tf.keras.metrics.Recall(),\n",
    "                  loss=tf.keras.losses.SquaredHinge(reduction=\"auto\", name=\"squared_hinge\")):\n",
    "\n",
    "    model = Sequential()\n",
    "    # Embedding layer\n",
    "    model.add(\n",
    "        Embedding(input_dim=len(vocab) + 1,\n",
    "                  input_length=input_length,\n",
    "                  output_dim=300,\n",
    "                  weights=[embedding_matrix],\n",
    "                  trainable=False,\n",
    "                  mask_zero=True))\n",
    "\n",
    "    # Masking layer for pre-trained embeddings\n",
    "    model.add(Masking(mask_value=0.0))\n",
    "\n",
    "    # Recurrent layer\n",
    "    model.add(LSTM(recurrent_layer_size, return_sequences=False, dropout=dropout, recurrent_dropout=recurrent_dropout,\n",
    "                   input_shape=(2048, 28, 300)))\n",
    "    # model.add(LSTM(recurrent_layer_size, return_sequences=False, dropout=dropout, recurrent_dropout=recurrent_dropout))\n",
    "\n",
    "    # # Second layer (batch_size, sequence_length, features)\n",
    "    # model.add(LSTM(int(recurrent_layer_size / 4), return_sequences=True))\n",
    "    # model.add(LSTM(int(recurrent_layer_size / 8), return_sequences=False))\n",
    "\n",
    "    # Fully connected layer\n",
    "    model.add(Dense(dense_size, activation=dense_activation))\n",
    "\n",
    "    # Dropout for regularization\n",
    "    model.add(Dropout(dropout_for_regularization))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(3, activation=output_activation))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=[result_fun]\n",
    "        )\n",
    "    return model\n",
    "\n",
    "\n",
    "# CHANGE PATH FOR SERVER\n",
    "local = \"/home/ivan/Documents/git_repos/Sentiment-Analysis-on-Twitter/models/model.h5\"\n",
    "djurdja = '/home/ikrizanic/pycharm/zavrsni/models/model.h5'\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=20),\n",
    "             ModelCheckpoint(local)]\n",
    "\n",
    "\n",
    "def fit_model(model, X_train, y_train, X_val, y_val, batch_size=2048, epochs = 10):\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        batch_size=batch_size, epochs=epochs,\n",
    "                        #callbacks=callbacks,\n",
    "                        validation_data=(X_val, y_val))\n",
    "    return history, model\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    res = model.evaluate(X_test, y_test)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T09:23:18.676870Z",
     "start_time": "2020-05-21T09:23:18.569250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading pickle data...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "local_path = \"/home/ivan/Documents/git_repos/Sentiment-Analysis-on-Twitter/data\"\n",
    "djurdja_path = \"/home/ikrizanic/pycharm/zavrsni/data\"\n",
    "working_path = local_path\n",
    "data_paths = {\"train_dataset\": working_path + \"/lstm/train_dataset.pl\",\n",
    "              \"input_dataset\": working_path + \"/lstm/input_dataset.pl\",\n",
    "              \"input_labels\": working_path + \"/lstm/input_labels.pl\",\n",
    "              \"test_dataset\": working_path + \"/lstm/test_dataset.pl\",\n",
    "              \"train_labels\": working_path + \"/lstm/train_labels.pl\",\n",
    "              \"test_labels\": working_path + \"/lstm/test_labels.pl\",\n",
    "              \"embedding_matrix\": working_path + \"/lstm/embedding_matrix.pl\"}\n",
    "\n",
    "dataset_name = \"main\" + \"_data\"\n",
    "djurdja_paths = {\"dataset\": str(\"~/pycharm/zavrsni/data/\" + dataset_name + \".csv\"),\n",
    "                 \"labels\": \"/home/ikrizanic/pycharm/zavrsni/data/labels.txt\"}\n",
    "local_paths = {\n",
    "    \"dataset\": str(\"/home/ivan/Documents/git_repos/Sentiment-Analysis-on-Twitter/data/\" + dataset_name + \".csv\"),\n",
    "    \"labels\": \"/home/ivan/Documents/git_repos/Sentiment-Analysis-on-Twitter/data/labels.txt\"}\n",
    "\n",
    "train_dataset = pd.read_csv(local_paths[\"dataset\"], sep=\"\\t\", names=[\"label\", \"text\"])\n",
    "# train_dataset = pd.read_csv(\n",
    "# '/home/ivan/Documents/git_repos/Sentiment-Analysis-on-Twitter/data/SemEval2017-task4-dev.subtask-A.english.INPUT.txt',\n",
    "#                             sep=\"\\t\", quotechar='\"',\n",
    "#                             names=[\"id\", \"label\", \"text\"])\n",
    "\n",
    "test_dataset = pd.read_csv(\n",
    "    '/home/ivan/Documents/git_repos/Sentiment-Analysis-on-Twitter/data/SemEval2017-task4-test.subtask-A.english.txt',\n",
    "    sep=\"\\t\", quotechar='\\'', names=[\"id\", \"label\", \"text\"])\n",
    "#\n",
    "# train_dataset, train_labels = return_tweets_and_labels(train_dataset)\n",
    "# test_dataset, test_labels = return_tweets_and_labels(test_dataset)\n",
    "#\n",
    "# train_dataset = process_dataset(train_dataset)\n",
    "# test_dataset = process_dataset(test_dataset)\n",
    "#\n",
    "# dump_dataset(train_dataset, data_paths[\"train_dataset\"])\n",
    "# dump_dataset(test_dataset, data_paths[\"test_dataset\"])\n",
    "# dump_labels(train_labels, data_paths[\"train_labels\"])\n",
    "# dump_labels(test_labels, data_paths[\"test_labels\"])\n",
    "\n",
    "print(\"Reading pickle data...\")\n",
    "train_dataset = load_dataset(data_paths[\"train_dataset\"])\n",
    "test_dataset = load_dataset(data_paths[\"test_dataset\"])\n",
    "train_labels = load_labels(data_paths[\"train_labels\"])\n",
    "test_labels = load_labels(data_paths[\"test_labels\"])\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T09:24:46.524421Z",
     "start_time": "2020-05-21T09:23:21.574999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train vocab and data encoding...\n",
      "Done\n",
      "Padding features...\n",
      "Done\n",
      "Ebedding matrix...\n",
      "Converted 32269 words (0 misses)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Train vocab and data encoding...\")\n",
    "train_vocab, enc_train_data = create_vocab_encode_data([d[\"anot_tokens\"] for d in train_dataset])\n",
    "enc_test_data = encode_data([d[\"anot_tokens\"] for d in test_dataset], train_vocab)\n",
    "print(\"Done\")\n",
    "\n",
    "print(\"Padding features...\")\n",
    "train_features = pad_encoded_data(enc_train_data, max(x for x in [len(d) for d in enc_train_data]))\n",
    "test_features = pad_encoded_data(enc_test_data, max(x for x in [len(d) for d in enc_train_data]))\n",
    "print(\"Done\")\n",
    "\n",
    "print(\"Ebedding matrix...\")\n",
    "embedding_matrix = make_embedding_matrix(train_vocab)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T09:24:46.687187Z",
     "start_time": "2020-05-21T09:24:46.670203Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_features, train_labels, test_size=0.1, random_state=25)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "max_len = max(x for x in [len(d) for d in enc_train_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T15:30:14.097034Z",
     "start_time": "2020-05-20T15:30:14.092357Z"
    }
   },
   "source": [
    "## Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T09:31:02.666538Z",
     "start_time": "2020-05-21T09:31:02.507239Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected sequence or array-like, got <class 'tensorflow.python.framework.ops.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/Programs/anaconda3/envs/sent/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/envs/sent/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                     \u001b[0;34m\"Please call `x.shape` rather than `len(x)` for \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m                     \"shape information.\".format(self.name))\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: len is not well defined for symbolic Tensors. (dense_28/Softmax:0) Please call `x.shape` rather than `len(x)` for shape information.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-823463e2339b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m                       \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                       \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresult_fun\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                       loss=tf.keras.losses.SquaredHinge(reduction=\"auto\", name=\"squared_hinge\"))\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-6f319ab663a0>\u001b[0m in \u001b[0;36mcompile_model\u001b[0;34m(vocab, embedding_matrix, input_length, trainable, recurrent_layer_size, dense_size, dropout, recurrent_dropout, dense_activation, dropout_for_regularization, output_activation, optimizer, metrics, loss)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresult_fun\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         )\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/envs/sent/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/envs/sent/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mskip_target_masks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_functions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             masks=masks)\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;31m# Compute total loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/envs/sent/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_handle_metrics\u001b[0;34m(self, outputs, targets, skip_target_masks, sample_weights, masks)\u001b[0m\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m                 self._handle_per_output_metrics(\n\u001b[0;32m--> 871\u001b[0;31m                     self._per_output_metrics[i], target, output, output_mask)\n\u001b[0m\u001b[1;32m    872\u001b[0m                 self._handle_per_output_metrics(\n\u001b[1;32m    873\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_per_output_weighted_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/envs/sent/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_handle_per_output_metrics\u001b[0;34m(self, metrics_dict, y_true, y_pred, mask, weights)\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m                 training_utils.call_metric_function(\n\u001b[0;32m--> 842\u001b[0;31m                     metric_fn, y_true, y_pred, weights=weights, mask=mask)\n\u001b[0m\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m     def _handle_metrics(self,\n",
      "\u001b[0;32m~/Programs/anaconda3/envs/sent/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcall_metric_function\u001b[0;34m(metric_fn, y_true, y_pred, weights, mask)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m         \u001b[0mupdate_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For TF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m             \u001b[0mmetric_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/envs/sent/lib/python3.6/site-packages/keras/utils/metrics_utils.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(metric_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;34m\"\"\"Decorated function with `add_update()`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mupdate_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_state_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mmetric_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdate_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/envs/sent/lib/python3.6/site-packages/keras/metrics.py\u001b[0m in \u001b[0;36mupdate_state\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze_or_expand_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         return super(MeanMetricWrapper, self).update_state(\n\u001b[1;32m    320\u001b[0m             matches, sample_weight=sample_weight)\n",
      "\u001b[0;32m<ipython-input-18-4b4f13d66a9e>\u001b[0m in \u001b[0;36mresult_fun\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mresult_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/envs/sent/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mrecall_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1787\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'recall'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m                                                  \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1789\u001b[0;31m                                                  zero_division=zero_division)\n\u001b[0m\u001b[1;32m   1790\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/envs/sent/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0;32m-> 1484\u001b[0;31m                                     pos_label)\n\u001b[0m\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/envs/sent/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1299\u001b[0m                          str(average_options))\n\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1302\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'binary'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/envs/sent/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \"\"\"\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/envs/sent/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \"\"\"\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/envs/sent/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \"\"\"\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/envs/sent/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected sequence or array-like, got <class 'tensorflow.python.framework.ops.Tensor'>"
     ]
    }
   ],
   "source": [
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "sq_hinge = tf.keras.losses.SquaredHinge(reduction=\"auto\", name=\"squared_hinge\")\n",
    "model = compile_model(train_vocab, embedding_matrix, max_len,\n",
    "                      recurrent_layer_size=128,\n",
    "                      dense_size=128,\n",
    "                      dropout=0,\n",
    "                      recurrent_dropout=0,\n",
    "                      dense_activation='relu',\n",
    "                      dropout_for_regularization=0.5,\n",
    "                      output_activation='softmax',\n",
    "                      optimizer='Adam',\n",
    "                      metrics=result_fun,\n",
    "                      loss=tf.keras.losses.SquaredHinge(reduction=\"auto\", name=\"squared_hinge\"))\n",
    "\n",
    "history, model = fit_model(model, X_train, y_train, X_val, y_val, batch_size=2048, epochs=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-20T21:39:02.337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2400/12258 [====>.........................] - ETA: 8s"
     ]
    }
   ],
   "source": [
    "res = model.evaluate(test_features, test_labels)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T21:21:27.268215Z",
     "start_time": "2020-05-20T21:21:22.588846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6180819602872835\n",
      "0.7293740509532647\n",
      "0.5300201816347124\n",
      "62.58253976250868\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = model.predict(test_features)\n",
    "\n",
    "\n",
    "c = np.eye(3, dtype=float)[np.argmax(predictions, axis=1)]\n",
    "\n",
    "tp, tn, tu, np, nn, nu = 0, 0, 0, 0, 0, 0\n",
    "for i in range(len(predictions)):\n",
    "    p = list(c[i])\n",
    "    l = list(test_labels[i])\n",
    "\n",
    "    if l == [1.0,0.0,0.0]:\n",
    "        nn += 1\n",
    "        if l == p:\n",
    "            tn += 1\n",
    "    if l == [0.0,1.0,0.0]:\n",
    "        nu += 1\n",
    "        if l == p:\n",
    "            tu += 1\n",
    "    if l == [0.0,0.0,1.0]:\n",
    "        np += 1\n",
    "        if l == p:\n",
    "            tp += 1\n",
    "    \n",
    "rp = tp/np\n",
    "ru = tu/nu\n",
    "rn = tn/nn\n",
    "print(rp)\n",
    "print(ru)\n",
    "print(rn)\n",
    "print((rp + rn + ru) * 100 / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:40:13.048170Z",
     "start_time": "2020-05-20T20:40:13.044818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2207\n",
      "3834\n",
      "1351\n",
      "3964\n",
      "5927\n",
      "2367\n",
      "0.5707646810308408\n",
      "0.6468702547663236\n",
      "0.556760847628658\n",
      "59.14652611419408\n"
     ]
    }
   ],
   "source": [
    "print(tn)\n",
    "print(tu)\n",
    "print(tp)\n",
    "print(nn)\n",
    "print(nu)\n",
    "print(np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:18:41.568492Z",
     "start_time": "2020-05-20T20:18:41.566265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7892\n",
      "4366\n",
      "0.6438244411812694\n"
     ]
    }
   ],
   "source": [
    "print(good)\n",
    "print(bad)\n",
    "print(good / (good + bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
