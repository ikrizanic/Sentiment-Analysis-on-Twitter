{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import re\n",
    "\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "import codecs\n",
    "from nltk.tokenize import PunktSentenceTokenizer,sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"wordnet\")\n",
    "\n",
    "#Spacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading extra data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_merge = pd.read_csv('../data/merge_bez2016.csv', sep=\"\\t\", names=[\"label\", \"text\"])\n",
    "emoticons_file = pd.read_csv('../data/emoticons.txt', sep=\"  ->  \", names=[\"emoji\", \"meaning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40256/40256 [00:02<00:00, 18268.52it/s]\n"
     ]
    }
   ],
   "source": [
    "def remove_duplicates(csv_data):\n",
    "    clean_data = dict()\n",
    "    for i in tqdm((range(len(csv_data)))):\n",
    "        if csv_data.text[i] not in clean_data and isinstance(csv_data.text[i], str):\n",
    "            clean_data.update({csv_data.text[i]: csv_data.label[i]})\n",
    "    return clean_data\n",
    "\n",
    "data = remove_duplicates(raw_data_merge)\n",
    "\n",
    "tweets = list()\n",
    "polarities = list()\n",
    "\n",
    "for text in data.keys():\n",
    "    tweets.append(text)\n",
    "    \n",
    "for polarity in data.values():\n",
    "    polarities.append(polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import queue\n",
    "vectorizer = CountVectorizer()\n",
    "vocab_pos = queue.PriorityQueue()\n",
    "vocab_net = queue.PriorityQueue()\n",
    "vocab_neg = queue.PriorityQueue()\n",
    "data, data_pos, data_net, data_neg = tweets, list(), list(), list()\n",
    "for i in range(len(tweets)):\n",
    "    if polarities[i] == \"positive\":\n",
    "        data_pos.append(tweets[i])\n",
    "    elif polarities[i] == \"neutral\":\n",
    "        data_net.append(tweets[i])\n",
    "    else:\n",
    "        data_neg.append(tweets[i])\n",
    "        \n",
    "        \n",
    "transformed_data = vectorizer.fit_transform(data)\n",
    "        \n",
    "transformed_data_pos = vectorizer.fit_transform(data_pos)\n",
    "transformed_data_net = vectorizer.fit_transform(data_net)\n",
    "transformed_data_neg = vectorizer.fit_transform(data_neg)\n",
    "\n",
    "for word, count in zip(vectorizer.get_feature_names(), np.ravel(transformed_data.sum(axis=0))): \n",
    "    vocab.put((int(count), word)) \n",
    "\n",
    "for word, count in zip(vectorizer.get_feature_names(), np.ravel(transformed_data_pos.sum(axis=0))): \n",
    "    vocab_pos.put((int(count), word)) \n",
    "for word, count in zip(vectorizer.get_feature_names(), np.ravel(transformed_data_net.sum(axis=0))): \n",
    "    vocab_net.put((int(count), word))\n",
    "for word, count in zip(vectorizer.get_feature_names(), np.ravel(transformed_data_neg.sum(axis=0))): \n",
    "    vocab_neg.put((int(count), word)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTICONS = dict()\n",
    "for i in range(len(emoticons_file)):\n",
    "    e = emoticons_file.emoji[i]\n",
    "    m = emoticons_file.meaning[i]\n",
    "    EMOTICONS.update({e:m})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoticons_distribution(tweets, polarities):\n",
    "    distribution = dict()\n",
    "    count = 0\n",
    "    [distribution.update({emot: [0,0,0,0]}) for emot in EMOTICONS.keys()]\n",
    "    for i in tqdm(range(len(tweets))):\n",
    "        tweets[i] = remove_links(tweets[i])\n",
    "        tweets[i] = repaire_chars(tweets[i])\n",
    "        tweets[i] = remove_usernames(tweets[i])\n",
    "        flag = 0\n",
    "        for emot in EMOTICONS:\n",
    "            if tweets[i].find(emot) != -1:\n",
    "                flag = 1\n",
    "                distribution[emot][3] += 1\n",
    "#                 print(emot + \"   ->   \"  + dataset[i].text[0])\n",
    "                if polarities[i] == \"positive\":\n",
    "                    distribution[emot][0] += 1\n",
    "                elif polarities[i] == \"neutral\":\n",
    "                    distribution[emot][1] += 1\n",
    "                else:\n",
    "                    distribution[emot][2] += 1\n",
    "\n",
    "        if flag != 1:\n",
    "            count += 1\n",
    "    print(\"Sentences without emoticons: \" + str(count * 100 / len(tweets)) + \"%\")\n",
    "    return distribution\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_useful_emoticons(tweets, polarities):\n",
    "    distribution = emoticons_distribution(tweets, polarities)\n",
    "    emoticons_score = dict()\n",
    "    good_scores = dict()\n",
    "    for k,v in distribution.items():\n",
    "        if v[3] == 0:\n",
    "            score = 0\n",
    "        else:\n",
    "            score = (v[0] - v[2]) / v[3]\n",
    "        emoticons_score.update({k:score})\n",
    "\n",
    "    for k,v in emoticons_score.items():\n",
    "        if v < -0.1 or v > 0.1:\n",
    "            good_scores.update({k:v})\n",
    "\n",
    "    return good_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39608/39608 [00:01<00:00, 30624.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences without emoticons: 94.39507170268632%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "useful_emoticons = find_useful_emoticons(tweets, polarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(raw):\n",
    "    return re.sub(r'http.*\\b', '[URL]', raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repairing some lost characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repaire_chars(raw):\n",
    "    raw = re.sub(r'\\\\u2019', \"\\'\", raw)\n",
    "    raw = re.sub(r'\\\\u002c', ',', raw)\n",
    "    raw = re.sub(r'&lt', '>', raw)\n",
    "    raw = re.sub(r'&gt', '<', raw)        \n",
    "    raw = re.sub(r'&amp;', '&', raw)\n",
    "    raw = re.sub(r'\\\\\\\"\\\"', '', raw)\n",
    "    raw = re.sub(r'\\\"\\\"\\\\', '', raw)\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_usernames(raw):\n",
    "    return re.sub(r'@[^\\s]*', '[USER]', raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing unuseful emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unuseful_emoticons(raw):\n",
    "    for k,v in EMOTICONS.items():\n",
    "        if k in useful_emoticons.keys() and k in raw:\n",
    "            raw.replace(k,v)\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotation and normalization (from github)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "        'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=True,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[EMOTICONS, emoticons]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotation_normalization(raw):\n",
    "    return \" \".join(text_processor.pre_process_doc(raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell check (from github)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/ivan/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from ekphrasis.classes.spellcorrect import SpellCorrector\n",
    "from nltk.corpus import words\n",
    "sp = SpellCorrector(corpus=\"english\") \n",
    "nltk.download(\"words\")\n",
    "words = set(words.words())\n",
    "def spell_check(raw):\n",
    "    correct_raw = list()\n",
    "    for word in raw.split(\" \"):\n",
    "        if(word in words):\n",
    "            correct_raw.append(word)\n",
    "        else:\n",
    "            correct_raw.append(sp.correct(word))\n",
    "    return \" \".join(correct_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posttokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(raw, tokenized):\n",
    "    stop_words_set = set(stopwords.words('english'))\n",
    "    tokens = []\n",
    "    for token in tokenized:\n",
    "        token_lower = token if token.islower() else token.lower()\n",
    "        if token_lower not in stop_words_set:\n",
    "            tokens.append(token)\n",
    "    return (raw, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T17:00:14.302289Z",
     "start_time": "2020-04-06T17:00:06.407463Z"
    }
   },
   "outputs": [],
   "source": [
    "from podium.datasets import Iterator\n",
    "from podium.storage import Vocab, Field, LabelField, MultioutputField\n",
    "from podium.storage.vectorizers.tfidf import CountVectorizer\n",
    "from podium.datasets import TabularDataset\n",
    "import functools\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg', disable=[\"parser\", \"ner\"])\n",
    "\n",
    "def extract_text_hook(raw, tokenized):\n",
    "    return raw, [token.text for token in tokenized]\n",
    "\n",
    "def extract_pos_hook(raw, tokenized):\n",
    "    return raw, [token.pos_ for token in tokenized]\n",
    "\n",
    "def extract_vec_hook(raw, tokenized):\n",
    "    return raw, [token.vector_norm for token in tokenized]\n",
    "\n",
    "\n",
    "text = Field(name='text', vocab=Vocab(), store_as_raw=True)\n",
    "text.add_posttokenize_hook(extract_text_hook)\n",
    "\n",
    "pos = Field(name='pos', vocab=Vocab())\n",
    "pos.add_posttokenize_hook(extract_pos_hook)\n",
    "\n",
    "vec = Field(name='vec')\n",
    "vec.add_posttokenize_hook(extract_vec_hook)\n",
    "\n",
    "text = MultioutputField([text, pos, vec], tokenizer=nlp)\n",
    "\n",
    "label = LabelField(name='label')\n",
    "fields = {'text': text, 'label':label}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T17:05:32.157366Z",
     "start_time": "2020-04-06T17:00:45.251652Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = TabularDataset('../data/merge_bez2016.csv', format='tsv', fields=fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretokenization hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.add_pretokenize_hook(repaire_chars)\n",
    "text.add_pretokenize_hook(annotation_normalization)\n",
    "text.add_pretokenize_hook(replace_unuseful_emoticons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posttokenization hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.add_posttokenize_hook(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T17:06:07.575755Z",
     "start_time": "2020-04-06T17:06:07.573825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example[label: ('positive', None); pos: (None, ['PUNCT', 'VERB', 'ADJ', 'PRON', 'CCONJ', 'DET', 'NOUN', 'VERB', 'NOUN', 'NUM', 'SYM', 'NUM', 'ADP', 'SYM', 'PROPN', 'PUNCT', 'ADP', 'NOUN', 'NUM', 'NOUN', 'PROPN', 'NUM', 'PUNCT', 'SPACE', 'PROPN', 'NUM', 'CCONJ', 'NUM', 'AUX', 'DET', 'ADJ', 'PUNCT']); text: ('@hughhefner Make sure you & the girls watch seasons 1-5 of #Dexter, before season 6 premieres Oct 2.  Seasons 1 & 4 are the best!', ['@hughhefner', 'Make', 'sure', 'you', '&', 'the', 'girls', 'watch', 'seasons', '1', '-', '5', 'of', '#', 'Dexter', ',', 'before', 'season', '6', 'premieres', 'Oct', '2', '.', ' ', 'Seasons', '1', '&', '4', 'are', 'the', 'best', '!']); vec: (None, [0.0, 5.0838113, 4.9412875, 5.1979666, 5.9343824, 4.70935, 6.974511, 6.519909, 7.012366, 5.269974, 5.6033444, 5.069743, 4.97793, 6.7399955, 6.29456, 5.094723, 5.2121177, 7.2339334, 5.067633, 6.335264, 5.5206485, 5.163114, 4.9316354, 0.0, 7.012366, 5.269974, 5.9343824, 5.05721, 5.41568, 4.70935, 5.2471824, 5.620569])]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "352px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 440.166666,
   "position": {
    "height": "40px",
    "left": "1396px",
    "right": "20px",
    "top": "72px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
