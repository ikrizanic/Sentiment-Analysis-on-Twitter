\documentclass[times, utf8, zavrsni]{fer}
\usepackage{booktabs}
\usepackage{glossaries}
\renewcommand{\vec}[1]{\mathbf{#1}}
\usepackage{graphicx}
\graphicspath{ {./images/}}
\usepackage{subfig}
\usepackage{pgfplots}
\pgfplotsset{width=10cm,compat=1.9}

\makeglossaries


\newglossaryentry{LSTM}
{
    name=LSTM,
    description={ćelija s dugoročnom memorijom (engl.~\emph{Long short-term memory})}
}

\newglossaryentry{SVM}
{
    name=SVM,
    description={stroj potpornih vektora (engl.~\emph{Support-vector machine})}
}

\newglossaryentry{CNN}
{
	name=CNN,
	description={konvolucijska neuronska mreža (engl.~\emph{Convolutional neural network})}
}

\newglossaryentry{RNN}
{
	name=RNN,
	description={povratna neuronska mreža (engl.~\emph{Reccurent neural network})}
}
\newglossaryentry{NLTK}{
name=NLTK,
description={knjižnica za obradu prirodnog jezika (engl~\emph{Natural Language Toolkit})}
}

\begin{document}

% TODO: Navedite broj rada.
\thesisnumber{7025}

% TODO: Navedite naslov rada.
\title{Strojno učenje za analizu sentimenta u mikroblogovima}

% TODO: Navedite vaše ime i prezime.
\author{Ivan Križanić}

\maketitle

% Ispis stranice s napomenom o umetanju izvornika rada. Uklonite naredbu \izvornik ako želite izbaciti tu stranicu.
\izvornik

% Dodavanje zahvale ili prazne stranice. Ako ne želite dodati zahvalu, naredbu ostavite radi prazne stranice.
\zahvala{ZAHVALA}

\tableofcontents

\chapter{Uvod}


Mikroblogovi su danas jedan od najčešće korištenih i najčešće proučavanih oblika komunikacije na internetu. Pronalaze se na iznimno popularnim društvenim mrežama, kao što su Twitter i Facebook, koje broje milijune korisnika diljem svijeta. Ljudi ih objavljuju u stvarnom vremenu, izražavajući svoje osjećaje, stavove i razmišljanja o svakodnevnom životu. Mnogi događaji i pojave u svijetu dobro su popraćeni reakcijama na društvenim mrežama, stoga je korisno proučavati velike skupove objava kao izvor stajališta, preferenci, osjećaja i mnogih drugih svojstava koja se daju izvući iz teksta objave. 

Ovaj se rad bazira na mikroblogovima društvene platforme Twitter. Takozvani Tweetovi, mikroblogovi platforme Twitter, kratke su poruke sačinjene od najviše $140$ znakova. Prvi je objavljen 2005. godine, a dvije godine kasnije dnevno se objavljivalo $5000$ mikroblogova. Po zadnjim poznatim podatcima taj broj iznosi preko $500$ milijuna objava dnevno \citep{twitterStats}. Radi se o iznimno velikom broju podataka koji kao skup mogu nositi korisne informacije, stoga ne čudi da postoje tvrtke koje u ponudi imaju analizu mikroblogova s Twittera i drugih društvenih platformi (\textit{brandmentions.com, mention.com}). Povratna informacija korisnika vrijedan je resurs kojim se tvrtke mogu opskrbiti, stoga analiza društvenih platformi ima velik ekonomski i društveni značaj. Obradi tako velikog broja podataka pristupa se tehnikama strojnog učenja, a konkretno područje koje se primjenjuje za ovakve zadatke naziva se obrada prirodnog jezika, ili još preciznije, analiza sentimenta. 

Rad se bavi problemom klasifikacije mikroblogova na one pozitivnog, neutralnog i negativnog sentimenta. Zadatak odgovara podzadatku A, četvrtog zadatka na natjecanju \emph{Semeval 2017}, koji je u vrijeme održavanja privukao $39$ timova iz cijelog svijeta. Ta je godina bila peta u nizu na kojoj se pojavio isti zadatak, što pokazuje interes zajednice za problem analize sentimenta. U sklopu zadatka napravljene su dvije implementacije modela za klasifikaciju. Jedna pripada standardnom strojnom učenju i temelji se na \gls{SVM}-modelu s linearnom jezgrom, a druga pripada području dubokog učenja i temelji se na povratnoj neuronskoj mreži (engl.~\emph{Reccurent neural network}, \gls{RNN}) s arhitekturom ćelije s dugoročnom memorijom (engl.~\emph{Long short-term memory}, \gls{LSTM}).

Rad je strukturno podijeljen na sljedeći način. U prvom se dijelu nalazi osvrt na radove koji su utjecali na ovaj rad, odnosno glavne izvore koji su bili motivacija za implementacije oba pristupa. U drugom dijelu osvrće se na implementaciju u ovom radu. Prvo se objašnjava odabir pristupa, a zatim i modeli korišteni u pristupima. Također su objašnjenje tehnologije korištene u obradi ulaznih podataka, te konačno i izrada značajki korištenih u treniranju modela. Treći dio rada opisuje podatke koji su korišteni i implementaciji, te prolazi kroz eksperimente i rezultate oba pristupa, da bi ih konačno i usporedio. Na kraju rada nalazi se zaključak i osvrt na moguća poboljšanja implementacije.

\chapter{Srodni radovi}


Na temu analize sentimenta napisano je mnogo radova, a velik broj bavi se upravo mikroblogovima s društvenih mreža i to vrlo često Twitterom. Uz to, u sklopu natjecanja \emph{Semeval} neki natjecatelji objavljuju i rad u kojem se osvrću na svoju implementaciju rješenja. Stoga je dostupno puno informacija koje se mogu iskoristiti za vlastitu implementaciju, ali je istovremeno i otežano implementirati neviđeno rješenje. 
%TODO update postotak točnosti
Najbolji rezultat implementacije u ovom radu ima točnost od $64.24\%$, što odgovara 10. mjestu na ljestvici predanih implementacija natjecanja \emph{Semeval 2017} \citep{semeval2017task4}. Prvo mjesto s točnošću od $68.1\%$ podijelila su dva tima: \textit{DataStories} i \textit{BB\_twtr}. Upravo je tim \textit{BB\_twtr} zaslužan za aktualan \textit{state-of-the-art} model u području analize sentimenta mikroblogova. Njihova trenutna implementacija hvali se da ostvaruje \textit{F1-score} u iznosu od $68.5\%$.

U sljedećih nekoliko odlomaka osvrće se na radove koji su služili kao izvor metoda i ostalih informacija koje su korištene u izradi ove implementacije.

\section{Rad tima \emph{BB\_twtr} - najuspješniji model današnjice}

Prvi u nizu radova na koje se treba osvrnuti jest rad pobjednika natjecanja \emph{Semeval 2017}, a ujedno i aktualni \textit{state-of-the-art} model u području analize sentimenta mikroblogova. Radi se o radu \textit{BB\_twtr at SemEval-2017 Task 4: Twitter Sentiment Analysis with CNNs and LSTMs} \citep{cliche-2017-bb}. Poblemu su pristupili tehnikama dubokog učenja. Prva faza rada bavi se izradom vektora riječi koji su dalje korišteni u treniranju CNN i LSTM modela mreža. Eksperimentirali su s tri različite tehnike izrade vektora riječi ( \textit{Word2Vec, FastText, GloVe}). U drugoj su fazi nenadziranim učenjem razdijelili sentiment na negativan i pozitivan, jer je prije toga sentiment polariteta u vektorima bio vrlo slab. U trećoj su fazi provodili nadzirano učenje koristeći podatke s natjecanja i model izgrađen od 10 \gls{CNN} i 10 \gls{RNN} mreža sa \gls{LSTM} arhitekturom koje koriste različit broj epoha za treniranje i različite vektore riječi. U podzadatku A postigli su točnost od $68.1\%$, a model su koristili i u ostala 4 podzadatka natjecanje te su u svim zadatcima ostvarili najbolji rezultat.

Budući da navode \gls{CNN} i \gls{RNN} (sa \gls{LSTM} arhitekturom) modele mreža kao najbolje u području analize sentimenta, u implementaciji ovog rada upotrebljava se \gls{RNN} model mreže sa \gls{LSTM} arhitekturom kako bi se upoznalo s njegovim mogućnostima. Umjesto izrade vektora riječi iz velikog skupa mikroblogova, koriste se gotovi vektori iz biblioteke \textit{Spacy} koja koristi vektore izrađene metodom \textit{Word2Vec}. Pri tome se gube prednosti posebnih značajki koje su karakteristične za jezik mikroblogova, a koje bi se mogle pokazati u vektorima nastalim na temelju mikroblogova, ali pristup je jednostavniji i štedi znatnu količinu računalne obrade koja bi bila potrebna za izradu vlastitih vektora.

\section{Rad tima \emph{DataStories}}

U podzadatku A natjecanja \emph{Semeval 2017}, zadatka 4, prvo mjesto dijelila su dva tima, ali tim \textit{DataStories} imao je niži \textit{F1-score}. Svoj su pristup opisali u radu \textit{DataStories at SemEval-2017 Task 4: Deep LSTM with Attention for Message-level and Topic-based Sentiment Analysis} \citep{datastories-Semeval}. S obzirom na to da su prethodnih godina ostvarili slabije rezultate, dok su timovi koji su koristili pristup dubokog učenja pretežno zauzeli pozicije na vrhu, \textit{DataStories} tim odlučio je skrenuti pažnju s klasičnog strojnog učenja na duboko učenje. Rad su podijelili na dva osnovna koraka: obradu teksta i treniranje modela. Za obradu teksta implementirali su vlastite funkcije koje su primjenjive u općoj upotrebi, ali su usmjerene na obradu mikroblogova s Twittera. Za izradu vektora riječi koristili su $330$ milijuna neoznačenih mikroblogova na engleskom jeziku. Na vokabularu od $660$ tisuća riječi koristili su \textit{GloVe} metodu izrade vektora. U obradi teksta koristili su vlastiti tokenizator koji je prilagođen Twitteru i posjeduje mogućnost izvlačenja raznih elemenata poput datuma, valuta, emotikona i sličnih sadržaja. Za razliku od njih, u svojoj implementaciji koristim implementaciju tokenizatora iz biblioteke \textit{SpaCy} jer je pristupačna i široko korištena. U daljnjoj su obradi primijenili standardne postupke pročišćavanja teksta koji se koriste u obradi prirodnog jezika.

Osvrnuli su se na konvolucijske mreže (\gls{CNN}) i naglasili problematiku gubitka informacije o poretku riječi prilikom uporabe istih. Iz tog su razloga preferirali \gls{RNN} s arhitekturom ćelije s dugoročnom memorijom (\gls{LSTM}).  U svojoj su implementaciji koristili dvoslojni dvosmjerni model s mehanizmom za pozornost koji pospješuje prepoznavanje korisnih težina. U \gls{LSTM} sloju modela koristili su $150$ neurona i trenirali s podskupovima od $128$ podataka. U testiranju su naveli kako mehanizam pozornosti doprinosi rezultatu za $0.04\%$ te stoga nije implementiran u modele ovog rada.


\section{Rad tima \emph{TakeLab} - pristup klasičnim strojnim učenjem}

Za razliku od velikog broja ekipa na natjecanju, tim \textit{TakeLab} odlučio se za pristup klasičnim metodama strojnog učenja. Koristili su skup ručno izrađenih značajki i trenirali na \gls{SVM}-modelu s linearnom jezgrom. Kao značajke koriste \emph{Tf-Idf} i gotove vektore riječi, ali i neke specifične značajke poput leksikona pozitivnih i negativnih riječi te posebnu značajku po kojoj je rad dobio ime: "\textit{Nedavne smrti i moć nostalgije}", odnosno originalni engleski naziv \textit{Recent Deaths and the Power of Nostalgia in Sentiment Analysis in Twitter} \citep{2017-takelab}. Značajka se temelji na činjenici da je sentiment mikroblogova koji spominju nedavno preminule ljude pretežno pozitivan jer se ljudi obično prisjećaju pozitivnih stvari vezanih za pokojnika. Također su iskoristili svojstva nostalgije koja upućuju na pretežno pozitivan sentiment prilikom spominjanja pojmova i pojava iz prijašnjih vremena. Za značajne ljude kreirali su značajke koje opisuju osobe s atributima svojstvenima njihovoj društvenoj ulozi, a za pojmove kojima je često pridjeljenje nekakva ocjena, npr. filmovi, igrice, glazba i slično, napravili su značajke koje donose informacije o uspješnosti i popularnosti pojma. Svojim \gls{SVM}-modelom ostvarili su solidan plasman u nekoliko zadataka, a u zadatku kojime se bavi ovaj rad ostvarili su 16. mjesto.

Zbog prisutnosti metode u njihovim radom, a i mnogim drugima koji koriste ručno izrađene značajke, u ovoj je implementaciji iskorišten leksikon pozitivnih i negativnih riječi i \gls{SVM}-model. Naprednije i inovativne značajke koje čine njihov rad posebnim nisu implementirane u model ovog rada.

\chapter{Implementacija}

\section{O zadatku 4 natjecanja \emph{Semeval 2017} i analizi sentimenta u mikroblogovima}

Verzija zadatka s kojim se ovaj rad bavi peta je u nizu na natjecanju \emph{Semeval}. Kao i svih prijašnjih godina, zadatak je bio poprilično popularan i privukao $48$ timova koji su sudjelovali u različitim podzadacima. U zadatku se kroz godine pojavilo nekoliko podzadatka kao što su ocjena pripadnosti sentimenta mikrobloga određenoj temi i skaliranje pripadnosti na skali od $1$ do $5$. Osnovna verzija zadatka bavi se klasifikacijom mikroblogova u tri razine polariteta, preciznije u mikroblogove pozitivnog, neutralnog i negativnog sentimenta. 

Analiza sentimenta u tekstovima kao što su mikroblogovi s društvenih mreža donosi razne poteškoće s kojima se ne mora nositi kada je riječ o standardnijim oblicima teksta. Problematične karakteristike mikroblogova su niska ograničenost broja znakova koja uzrokuje sažet izraz, ali i povećava uporabu kolokvijalnih izraza, skraćenica i raznih suvremenih novotvorenica koje bismo mogli okarakterizirati kao \emph{slang}. Prisutni su i razni elementi koji ne pripadaju prirodnom jeziku kao što su emotikoni, hiperlinkovi i razne oznake kao npr. oznaka korisničkog imena koja ima oblik \emph{"@user"}. Hiperlinkovi u takvim kratkim tekstovima često nose velik teret značenja, odnosno često tek uz informaciju o sadržaju na koji hiperlink pokazuje možemo pravilno ocijeniti sentiment same poruke. Problem je i u pravopisu, korisnici često mijenjaju riječi radi postizanje vizualnog ili nekog drugog efekta, pa tako možemo naići na tvorevine poput: "ŁoŁ", "ca\$h", "\copyright ool", i slične koje bi bilo poželjno prepoznati i pretvoriti u smislene riječi ili kratice. Također je prisutno nizanje istog slova u riječima poput "\emph{cool}" koje možemo pronaći u obliku kao što je "\emph{cooool}" ili negaciji \emph{"no"} kojoj se često nadodaje zadnje slovo \emph{"o"}. Takvim je riječima također poželjno ukloniti suvišne znakove kako bi se pronašle u rječniku, ali treba imati na umu da takvo ponavljanje znakova nosi značenje u sebi, a koje bismo klasičnim ispravljanjem pravopisa izgubili. Korisno bi bilo prepoznati i skrivene riječi kao što su \emph{"F**k"}. \emph{"S**t"}, \emph{"N***a"} jer su to često riječi koje mogu znatno utjecati na sentiment objave, no to nije tako jednostavan zadatak zbog raznih metoda kojima se takve, često proste riječi, pokušavaju ukomponirati u tekstove.

Kada se odmakne od početne obrade teksta nailazi se na nove poteškoće kao što su korištenje sarkazma i učestalost ciničnog tona koji u potpunosti mijenjaju polaritet sentimenta, a koje je vrlo teško prepoznati iz perspektive modela. Ograničenost duljine poruke posljedično donosi manjkavost izraza koji se često bolje razumiju ako se posjeduje znanje o svijetu i vremenu u kojem su napisani, a ne samo o jeziku i značenju istog, što je još jedno svojstvo koje je vrlo teško ostvarivo modelima strojnog učenja. 


\section{Odabir metoda i pristupa}

Korišteni pristupi spadaju u metode strojnog učenja. Strojno se učenje dijeli na tri osnovne vrste: (1) nadzirano strojno učenje, (2) nenadzirano strojno učenje i (3) podržano strojno učenje. Razlika proizlazi iz prisutnosti oznaka podataka na kojima se vrši učenje, odnosno ako su podatci korišteni u npr.~klasifikaciji označeni s oznakama klase kojoj pripadaju, onda je riječ o nadziranom strojnom učenju, dok se o nenadziranom strojnom učenju radi ako su oznake klasa odsutne tijekom cijelog procesa učenja. U ovom se radu koristi samo varijanta nadziranog strojnog učenja jer su svi podatci označeni.

U rješavanju problema koriste se dva različita pristupa kako bi se osvijestilo o prednostima i manama jednog i drugog. Prvi pristup pripada klasičnim metodama strojnog učenja i temelji se na vlastoručnoj izradi značajki i upotrebi \gls{SVM}-modela. Drugi pristup pripada grani strojnog učenja koja se naziva duboko učenje i  temelji se na značajkama nastalima od vektora riječi  i \gls{LSTM} inačici modela povratne neuronske mreže.



\subsection{Klasično strojno učenje - \gls{SVM}-model}

Strojevi potpornih vektora (engl.~\emph{Support-vector machine}) diskriminativni su modeli korišteni u nadziranom strojnom učenju, a koriste se u rješavanju klasifikacijskih i regresijskih problema \citep{strojno_skripta}. U klasifikaciji se izvorno koriste za binarnu klasifikaciju, stoga implementacija u ovom radu koristi posebnu modifikaciju na koju će se osvrnuti naknadno. \gls{SVM} rješava problem proizvoljnosti hipoteze uvođenjem kriterija maksimalne margine (engl.~\emph{maximum margin}). Naziv dolazi od takozvanih potpornih vektora koji su kombinacija odabranih vektora iz skupa za učenje, a koji omogućuju prikaz hiperravnine kod modela. Proširenje učinkovitosti \gls{SVM}-modela postiže se korištenjem jezgrenih funkcija postupkom trika jezgre (engl.~\emph{kernel trick}). \gls{SVM}-model je linearan: \[ h(\vec{x;w},w_0)=\vec{w}^\mathrm{T}\vec{x} + w_0, \] a za nelinearnost se može upotrijebiti preslikavanje $\phi$. Uz pretpostavku linearne odvojivosti i uzimajući da vrijedi $y\in\{-1,+1\}$ može se konstatirati da vrijedi: \[ \forall(\vec{x}^{(i)}, y^{(i)})\in D: y^{(i)}h(\vec{x}^{(i)})\ge 0.\] Riječima rečeno može se tvrditi da za svaki par vektora značajki i oznake klase vektora postoji predikcija $h(\vec{x})$ koja je istog predznaka kao oznaka klase, odnosno umnožak predikcije i oznake klase uvijek je veći ili jednak $0$. Ako su primjeri linearno odvojivi, postoji beskonačan broj rješenja koji zadovoljavaju navedeni izraz. Traži se rješenje maksimalne margine, što ima smisla jer je u interesu što bolje odvojiti klase primjera. Po definiciji je margina jednaka najmanjoj udaljenosti između $\vec{x}$ i hiperravnine, a cilj je pronaći za koju vrijednost $\vec{x}$ i $w_0$ margina ima maksimalan iznos, što vodi do sljedeće formule za izračun margine:\[ \underset{\mathbf{w}, w_{0}}{\operatorname{argmax}}\left\{\frac{1}{|\mathbf{w}|} \min _{i}\left\{y^{(i)}\left(\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}^{(i)}\right)+w_{0}\right)\right\}\right\}. \] Ovom je formulom teško riješiti optimizacijski problem, pa se stoga problem oblikuje u problem konveksne optimizacije. Uzimajući pretpostavku da za $x^{(i)}$ koji je najbliži margini vrijedi: \[ y^{(i)}\left(\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}^{(i)}\right)+w_{0}\right)=.1 \] Zbog toga mora vrijediti da za $\forall(\vec{x}^{(i)}, y^{(i)}) \in D$  vrijedi:\[ y^{(i)}\left(\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}^{(i)}\right)+w_{0}\right) \geqslant 1, \quad n=1, \ldots, N \] Za primjere za koje vrijedi jednakost kažemo da su ograničenja aktivna, dok za ostale kažemo da su ograničenja neaktivna. Maksimizirana margina ima barem dva aktivna ograničenja, što se može vidjeti na slici \ref{svm_pic}. Širina maksimalne margine iznosi $\frac{2}{||\vec{w}||}$, pa se problem optimizacije može svesti na maksimizaciju izraza: \[\underset{\vec{w}, w_{0}}{\operatorname{argmax}} \frac{1}{ ||\vec{w}||}. \] Kako bi se za optimizaciju mogla primijeniti metoda Lagrangeovih multiplikatora, izraz se piše kao: \[ \underset{\mathbf{w}, w_{0}}{\operatorname{argmin}} \frac{1}{2}\|\mathbf{w}\|^{2}, \] jer minimum $||\vec{w}||^2$ jednak je maksimumu $\frac{1}{||\vec{w}||}$.\\Time se problem svodi na problem kvadratno ograničenog kvadratnog programiranja (engl.~\emph{quadratically constrained quadratic programming)} i može se riješiti primjenom Lagrangeovog multiplikatora. Konačan izraz nastao kombiniranjem ciljne funkcije i uvjeta je sljedeća Lagrangeova funkcija: \[ L\left(\mathbf{w}, w_{0}, \boldsymbol{\alpha}\right)=\frac{1}{2}\|\mathbf{w}\|^{2}-\sum_{i=1}^{N} \alpha_{i}\left\{y^{(i)}\left(\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}^{(i)}\right)+w_{0}\right)-1\right\} .\] U daljnje postupke optimizacije ovaj rad ne ulazi, ali bitno je napomenuti da je rezultat optimizacije $N$-dimenzionalni vektor parametra $\alpha$ te da se klasifikacija neviđenog primjera vrši računanjem sljedećeg izraza, odnosno određujući njegov predznak: \[ h(\mathbf{x})=\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}(\mathbf{x})+w_{0}=\sum_{i=1}^{N} \alpha_{i} y^{(i)} \boldsymbol{\phi}(\mathbf{x})^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}^{(i)}\right)+w_{0}. \]

\begin{figure}
	\centering
	\includegraphics[width=0.65\textwidth]{svm2}
	\caption{Maksimalna margina }
	\label{svm_pic}
\end{figure}

\subsubsection{Problem višeklasne klasifikacije}

Standardnom upotrebom \gls{SVM}-model radi binarnu klasifikaciju, pa je za višeklasnu klasifikaciju $\left(K > 2\right)$ potrebno koristiti posebne metode. Osnovna ideja izvedbe jest modificirati problem višeklasne klasifikacije u više problema binarne klasifikacije. Postoji nekoliko načina na koji se ostvaruje željena modifikacija, a implementacija rješenja ma koju se rad osvrće koristi metodu jedan-naspram-ostali (engl.~\emph{one-vs-rest}) koja je zadana metoda knjižnice \emph{scikit-learn}. Princip se temelji na svođenju problema na $K - 1$ problema binarne klasifikacije, gdje je $K$ broj klasa koje početni problem može klasificirati. Tada svaki od $K-1$ binarnih klasifikatora $h_i$ odjeljuje klasu $C_i$ od svih preostalih klasa. Problem pristupa javlja se ako više klasifikatora klasificira primjer kao pozitivan za svoju klasu, jer tada nije moguće jednoznačno odrediti klasu kojoj primjer pripada.


\subsection{Duboko učenje}

Duboko učenje grana je strojnog učenja čiji se modeli temelje na neuronskim mrežama \citep{cupic}. Postoji mnogo modela i varijacija, a implementacija u ovom radu koristi povratnu neuronsku mrežu (engl.~\emph{Recurrent neural network}) s arhitekturom ćelije s dugoročnom memorijom (engl.~\emph{Long short-term memory}). Ostali poznati modeli neuronskih mreža su konvolucijske neuronske mreže (engl.~\emph{Convolutional neural network}), duboke neuronske mreže (engl.~\emph{Deep neural network}), duboka probabilistička mreža (engl.~\emph{Deep belief network}). Općenito, neuronske mreže složeni su sustavi čija je osnovna gradivna jedinica neuron čiji je zadatak propuštati težinsku sumu kroz prijenosnu funkciju određujući tako izlaznu vrijednost. Ulazi u neuron množe se s težinskim funkcijama $w_{1..n}$ te se sumiraju uz dodatak pomaka (engl.~\emph{bias}) $w_0$ daju vrijednost $z$ koja propuštanjem kroz prijenosnu funkciju daje izlaz $f(z)$ iz neurona. Primjer neurona prikazan je na slici \ref{neuron}.  


\begin{figure}[h]
	\centering
	\includegraphics[width=0.75\textwidth]{neuron}
	\caption{Umjetni neuron}
	\label{neuron}
\end{figure}

\subsubsection{Prijenosne funkcije}

Propusne funkcije mogu biti razne, a najčešće su sljedeće:
\begin{itemize}
	\item binarna step funkcija $f(x)=\left\{\begin{array}{l}0 \text { for } x<0 \\1 \text { for } x \geq 0\end{array}\right.$
	\item sigmoidalna funkcija $\sigma(x)=\frac{1}{1+e^{-x}}$
	\item ReLU funkcija $max(0,x)$
	\item tanges hiperbolički funkcija $\tanh(x)$
\end{itemize}
Linearno ponašanje nije karakteristično za prirodne pojave, pa stoga linearne prijenosne funkcije nisu pogodne za upotrebu u područjima poput računalnog vida ili obrade prirodnog jezika. Nelinearne funkcije omogućuju bolju procjenu prirodnih fenomena kao što jest jezik. Najkorištenije nelinearne funkcije su sigmoidalna funkcija, tangens funkcija i \emph{ReLU} funkcija. Sigmoidalna funkcija (slika \ref{aktivacijske}(a)) najčešće je korištena funkcija izlaznog sloja kada se radi o binarnoj klasifikaciji jer skuplja sve vrijednosti na interval $[0,1]$. Slična tome je i hiperbolička tangens funkcija (slika \ref{aktivacijske}(b)) koja radi na intervalu $[-1,1]$, ali zbog svoje derivacije pruža snažniji gradijent i možemo ju smatrati superiornijom u odnosu na sigmoidalnu funkciju \citep{akt}. Funkcija koja je najčešće prisutna u \gls{LSTM} arhitekturi \gls{RNN}-a je \emph{ReLU} prijenosna funkcija (slika \ref{aktivacijske}(c)) koja svim negativnim vrijednostima pridaje vrijednost 0, dok pozitivne vrijednosti slijede linearnu funkciju. Postoji varijacija te aktivacijske funkcije po imenu \emph{Leaky ReLU} koja za negativne vrijednosti slijedi funkciju $f(x) = 0.01x$, a za pozitivne se ponaša isto kao \emph{ReLu} i slijedi funkciju $f(x) = x$ ili eventualno $f(x) = kx$.

\begin{figure}
     \centering
     \subfloat[][sigmoidalna funkcija]{\includegraphics[width=0.2\textwidth]{sigm}\label{sigm}\hspace{7mm}}
     \subfloat[][tanh(x)]{\includegraphics[width=0.2\textwidth]{tanh}\label{tanh}\hspace{7mm}}
     \subfloat[][\emph{ReLU}]{\includegraphics[width=0.2\textwidth]{relu}\label{relu}}
     \caption{Aktivacijske funkcije}
     \label{aktivacijske}
\end{figure}

\subsubsection{Povratne neuronske mreže}
Arhitektura mreže koja je implementirana u radu je povratna neuronska mreža (\gls{RNN}) koja pripada porodici slojevitih unaprijednih neuronskih mreža. To znači da u sebi nema cikluse, odnosno ulazi neurona ne ovise o izlazima neurona koji se nalaze u dubljim slojevima. Karakteristika povratne neuronske mreže prisutnost je memorije. To im omogućuje obradu sekvencionalnih ulaza, odnosno mreže takvih arhitektura uzimaju u obzir poredak ulaznih podataka i time stvaraju povezanost između ulaza. Princip rada može se objasniti uz pomoć prikaza na slici \ref{rnnprincip}. Za ulaz $x_0$ u prvom koraku vrijedi da je ulaz jednak $x_0$, a za svaki sljedeći ulaz vrijedi da je jednak $x_{t} + h_{t-1}$, odnosno kombinaciji izlaza iz prethodnog koraka i trenutnog ulaza iz skupa ulaznih podataka. Problem koji se javlja u RNN-arhitekturi jest takozvani gubitak gradijenta, odnosno događa se da vrijednost u neuronu postaje toliko beznačajna da je daljnje treniranje gotovo onemogućeno \citep{rnn_training}. Tome se doskače korištenjem ćelije s dugoročnom memorijom (\gls{LSTM}).

\begin{figure}
	\centering
	\includegraphics[width=0.3\textwidth]{rnn}
	\caption{Princip ponašanja povratne neuronske mreže}
	\label{rnnprincip}
\end{figure}

\subsubsection{Arhitektura ćelije s dugoročnom memorijom}
Arhitektura ćelije s dugoročnom memorijom (\gls{LSTM}) prikazana je na slici \ref{lstmarh}. Sastoji se od triju ulaznih vrata:
\begin{itemize}
	\item ulazna vrata
	\item vrata za zaboravljanje
	\item izlazna vrata
\end{itemize}

Ulazna vrata odgovorna su za odabir vrijednosti koje će modificirati stanje u memoriji. Sastoje se od sigmoidalne i tangens hiperbolične funkcije. Sigmoidalna funkcija odgovorna je za odabir vrijednosti koje će sudjelovati u modifikaciji memorije, a tangens hiperbolični odgovoran je za pridjeljivanje odgovarajuće težine ulazu. Vrata za zaboravljanje odgovorna su za prebiranje vrijednosti iz prethodne iteracije i ulaznog podatka i također funkcioniraju na temelju sigmoidalne funkcije. Izlazna vrata određuju izlaz koristeći ulazni podatak i stanje u memoriji, a kada je riječ o prijenosnim funkcijama izvedba im je jednaka ulaznim vratima. Funkcionalnost \gls{LSTM} arhitekture zapisana jednadžbama je sljedeća: \[ \begin{aligned}
f_{t} &=\sigma_{g}\left(W_{f} x_{t}+U_{f} h_{t-1}+b_{f}\right) \\
i_{t} &=\sigma_{g}\left(W_{i} x_{t}+U_{i} h_{t-1}+b_{i}\right) \\
o_{t} &=\sigma_{g}\left(W_{o} x_{t}+U_{o} h_{t-1}+b_{o}\right) \\
\tilde{c}_{t} &=\sigma_{h}\left(W_{c} x_{t}+U_{c} h_{t-1}+b_{c}\right) \\
c_{t} &=f_{t} \circ c_{t-1}+i_{t} \circ \tilde{c}_{t} \\
h_{t} &=o_{t} \circ \sigma_{h}\left(c_{t}\right)
\end{aligned}, \]
gdje je $x_t$ vektor ulaznih podataka, $f_t$ aktivacijski vektor vrata za zaborav, $o_t$ aktivacijski vektor izlaznih vrata, $i_t$ aktivacijski vektor ulaznih vrata, $h_t$ izlazni vektor \gls{LSTM}-a, $\tilde{c}_t$ aktivacijski vektor ulaza u ćeliju, $c_t$ vektor stanja ćelije i konačno $W, U$ i $b$ matrice težina i pomaka koje je potrebno naučiti tijekom treniranja.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.65\textwidth]{lstm}
	\caption{\gls{LSTM} arhitektura}
	\label{lstmarh}
\end{figure}



\section{Obrada ulaznih podataka}

Zbog prirode mikroblogova ulazni su podatci prošarani raznim značajkama koje je potrebno ukloniti ili preinačiti. Pri stvaranju podataka pogodnih za izradu značajki nastale su dvije vrste podataka.\\
\noindent Prva je jednostavna i služi izradi značajki temeljenih na frekvencijama riječi ili vektorima riječi. Dobivena je tako što je prvo provedena zamjena emotikona s njihovom jezičnom reprezentacijom u smislu da je npr.~":-)" pretvoreno u \emph{"happy"}. Zatim je provedeno uklanjanje svih nejezičnih elemenata kao što su hiperlinkovi, korisnička imena, emotikoni, brojevi i slično. Mikroblogovi su rastavljeni na riječi koristeći biblioteku \emph{Spacy} te je nad dobivenim skupom jezičnih elemenata proveden postupak ispravljanja pravopisa i prepoznavanje žargona. Uklonjene su riječi koje ne doprinose značenju. Takve se riječi nazivaju zaustavne riječi (engl.~\emph{stop-words}) i uklonjene su koristeći biblioteku za obradu prirodnog jezika \emph{Natural Language Toolkit (\gls{NLTK})} \citep{nltk}. Na preostalim riječima proveden je postupak lematiziranja, odnosno pretvaranja riječi iz izvedenog oblika u njen korijenski oblik, takozvanu lemu. Za postupak lematiziranja korišten je \texttt{WordNetLemmatizer} iz spomenute biblioteke \gls{NLTK}.\\
\noindent Druga vrsta sastoji se od mikroblogova koji su označeni koristeći alat koji je napravio spomenuti tim \emph{DataStories} s natjecanja \emph{Semeval} \citep{datastories-Semeval}. U mikroblogu se ovim postupkom označavaju elementi poput hiperlinkova, cenzuriranih riječi, brojeva, riječi napisanih velikim slovima, korisničkih imena i slično. Takve su podatci ostavljeni u obliku teksta, odnosno nisu razlomljeni na manje elemente, jer su korišteni za prebrojavanje prisutnosti spomenutih elemenata.

\section{Značajke}

S obzirom na to da se paralelno koriste dva pristupa izrade i treninga modela, izrada i korištenje značajki također je podijeljena u dva smjera. Izrada značajki za klasično strojno učenje znatno je opsežniji i kreativniji proces nego izrada istih za pristup dubokim učenjem. Moglo bi se reći da je srž klasičnog pristupa upravo u izradi značajki jer se modeli sami po sebi ne mogu značajno konfigurirati, pa rezultat najviše ovisi o onome što mu se na ulazu pruži. Kod dubokog učenja postoje jednostavni standardni pristupi koji su ponekad gotovo mandatorni. 


\subsection{Značajke u klasičnom pristupu}

Značajke u ovom pristupu čine okosnicu uspjeha modela, pa je stoga izradi posvećen znatan udio vremena. Značajke se mogu grupirati u tri kategorije: \begin{itemize}
\item brojanje riječi i vektori riječi
\item polaritet i sentiment riječi
\item brojanje prisutnosti elemenata
\end{itemize}

\subsubsection{Brojanje riječi i vektori riječi}

U ovoj se kategoriji nalaze dvije vrste značajki koje se međusobno isključuju, odnosno ne koriste se istovremeno. Prva značajka je uobičajena kao početni uzorak značajki koji se koristi za treniranje osnovnog modela (engl.~\emph{baseline model}) kao referenca za daljnje eksperimente. Radi se 
se o metodi vreće riječi (engl. \emph{Bag-of-Words}), odnosno preciznije o primjeni mjere učestalosti riječi \emph{TF-IDF} (engl. \emph{term  frequency-inverse  document  frequency}). Mjera se definira na sljedeći način: potrebno je definirati dvije zasebne statističke mjere -- mjeru učestalosti izraza (\emph{tf}) i inverznu učestalost u dokumentima (\emph{idf}). Prvu mjeru koja označava učestalost pojave riječi računamo na sljedeći način: \[ \mathrm{tf}(t,d) = 0.5 + 0.5 \cdot  \frac{f_{t, d}}{\max\{f_{t', d}:t' \in d\}}\]
\noindent\\
gdje $t$ predstavlja izraz, $d$ predstavlja dokument, odnosno skup svih izraza u promatranom tekstu, $f_{t,d}$ predstavlja broj pojavljivanja izraza u tekstu, a brojnik predstavlja najveći broj pojavljivanja nekog izraza u tekstu. Ova se normalizirana verzija učestalosti pojave izraza koristi radi sprječavanja pristranosti prema velikim tekstovima.
Druga mjera koju određujemo je inverzna učestalost izraza u dokumentu i računa se na sljedeći način:
\[ \mathrm{idf}(t, D) =  \log \frac{N}{|\{d \in D: t \in d\}|} \]
\noindent\\
gdje $D$ predstavlja skup svih tekstova na kojima računamo učestalost, a nazivnik razlomka predstavlja broj pojavljivanja izraza u tekstu, dok je $N$ ukupan broj dokumenata u skupu $D$.
Konačna mjera jednaka je umnošku dviju prethodno izračunatih mjera, odnosno: \[ {\displaystyle \mathrm {tfidf} (t,d,D)=\mathrm {tf} (t,d)\cdot \mathrm {idf} (t,D)} \]
\noindent
U implementaciji rješenja koristim gotovu metodu iz knjižnice \emph{scikit-learn}. Korištenjem nastalih značajki u \gls{SVM}-modelu dobio sam točnost od $41.22\%$, što je korektan osnovni model s obzirom na točnost nasumičnog odabira koja iznosi $33.3\%$.\\\\
Druga značajka koju sam uveo, a koja pripada ovoj kategoriji, temelji se na vektorima riječi iz knjižnice \emph{Spacy}. Radi se o vektorima s 300 dimenzija nastalima primjenom \emph{Word2vec} \citep{w2v} metode. \emph{Word2vec} je model plitke neuronske mreže s dva sloja koji treniranjem pokušava rekonstruirati lingvističko značenje riječi. Kao ulaz koristi vrlo velik skup tekstualnih podataka koji mogu biti raznog porijekla, kao npr.~ članci \emph{Wkipedie}, objave na društvenim mrežama, primjerci elektroničke pošte itd.~Kao rezultat nastaju višedimenzionalni vektori čije se dimenzije obično kreću između $100$ i $1000$ dimenzija. Vektori su u prostoru smješteni tako da su vektori riječi bliskog znanja prostorno bliski jedan drugome.\\
Koristeći prethodno izrađene informacije proizašle iz metode \emph{tf-idf} kodirao sam ulazne informacije vektorima riječi tako da sam izračunao prosječnu vrijednost vektora svih riječi koje se pojavljuju u mikroblogu, a za broj značajki u tako nastalom vektoru odabrao sam vrijednost broja riječi u najduljem mikroblogu. Poboljšanje nastalo korištenjem ovih značajki značajno je povećalo uspješnost modela koji je nakon treniranja imao točnost od $60.9\%$

\subsubsection{Polaritet i sentiment riječi}

S obzirom na to da je zadatak klasifikacija s obzirom na polaritet mikrobloga, bilo je nužno dotaknuti se polariteta i sentimenta samih riječi. Za to su iskorištena dva leksikona. Prvi od njih je leksikon ocjena riječi koji sadrži ocjene po atributima zadovoljstva, uzbuđenosti i dominantnosti po imenu  \emph{Affective Norms for English Words} \citep{anew}. Prva verzija sastojala se od nešto više od tisući riječi, no 2013. godine proširena je na $14000$ \citep{anew2}. Pri implementaciji rješenja korišten je pristup temeljen na pristupu koji je prisutan u repozitoriju \emph{dwzhou/SentimentAnalysis} \citep{anew_code}. Drugi leksikon koji je korišten je zapravo običan popis pozitivnih i negativnih riječi \citep{leksikon}. 
\noindent Dodavanjem značajki dobivenih korištenjem leksikona podigao sam točnost modela za $1\%$, odnosno postigao točnost od $61.9\%$.

\subsubsection{Brojanje prisutnosti elemenata}

U ovoj se kategoriji nalaze značajke nastale brojanjem ili promatranjem prisutnosti raznih elemenata u mikroblogovima. Elementi čija je prisutnost označena zastavicama $0$ ili $1$ su: \emph{e-mail} adrese, hiperlinkovi, znakovi valute, datumi, telefonski brojevi itd.~Za neke se elemente bilježio točan broj pojavljivanja u mikroblogu, a izrazi za koje je bilježena ta informaciju su: izrazi s nizanjem znakova (npr.~\emph{"cooool"}), izrazi napisani velikim slovima, cenzurirani izrazi (npr.~\emph{"F***}), broj ponovljenih riječi, pojave takozvanih \emph{hashtagova} i broj uskličnika. Kao dodatna značajka nadodan je i ukupan broj riječi u rečenici.
\noindent Dodavanjem ovih značajki ostvaren je porast točnosti od $0.4\%$, odnosno postignuta je konačna točnost \gls{SVM}-modela od $62.32\%$.

\subsection{Značajke u dubokom učenju}

Izrada značajki korištenih u modelu dubokog učenja znatno je jednostavnija. Potrebno je izgraditi vokabular riječi koje se pojavljuju u skupu podataka i svakoj riječi u vokabularu pridijeliti redni broj koji će služiti kao oznaka riječi. Zatim je potrebno izgraditi matricu vektora riječi (engl.~\emph{embedding matrix}) koja se sastoji od onoliko stupaca koliko vektor riječi ima dimenzija, što je u slučaju ove implementacije 300 dimenzija. U redovima su po rednim brojevima iz vokabulara kodirane riječi odgovarajućim vektorima riječi. Model tijekom inicijalizacije prima matricu vektora. Pomoću izrađenog vokabulara vrši se kodiranje sadržaja mikroblogova, odnosno umjesto tekstualnog sadržaja mikroblogovi postaju vektori brojeva koji predstavljaju redni broj riječi u vokabularu. Radi konzistentnosti dimenzija odabrana je maksimalna veličina vektora koja odgovara najvećem vektoru u skupu podataka za treniranje, a svi manji vektori nadopunjavaju se nulama do željene veličine. Takav skup vektora predaje se modelu kao ulaz u treningu i u evaluaciji modela.

\chapter{Provedba eksperimenata}

Ovo poglavlje opisuje proces provođenja eksperimenata i osvrće se na postignute rezultate. Detaljno opisuje karakteristike implementacija i uspoređuje učinke izmjena modela koje su se činile tijekom eksperimentiranja, kao i podešavanje hiperparametara.

\section{Podatci}

Skup podataka za trening sastoji se od $49491$ mikroblogova, dok se skup podataka za testiranje sastoji od $12258$ mikroblogova. Podatci su odmah podijeljeni na one za treniranje i one za testiranje jer je sam skup podataka proizašao iz natjecanja \emph{Semeval 2017}, pa su korišteni originalni podatci za odgovarajuće faze natjecanja, tako da su rezultati postignuti na testnom skupu podataka mjerodavni onima koji su dobiveni kao rezultati natjecanja. Podatci se strukturno sastoje od identifikacijskih brojeva objava, oznake polariteta objave i teksta objave. Identifikacijske oznake izbačene su prilikom učitavanja jer niti jedna značajka ne proizlazi iz njih. 
Primjer jedne originalne objave:\\\\
\texttt{
\footnotesize{"(OFF TOPIC) - there is only 3 episodes on the first disk of \#Dexter. Please hurry, @netflix with the 2nd \#fitblog" }
}\\

\noindent Ti su podatci obradom poprimili oblik prikladan izvlačenju značajki, pa je prethodno spomenuta objava pretvorena u dvije vrste podataka. Prva vrsta jest popis riječi koje se nalaze u objavi, a koje ne pripadaju zaustavnim riječima (enlg.~\emph{stop-words}), a druga vrsta je tekst koji sadrži oznake bitnih elemenata i svojstava objave. Primjer obje vrste podataka:\\\\
\texttt{\footnotesize{['topic', 'episode', 'first', 'disk', 'dexter', 'please', 'hurry', \\'fit', 'web', 'log']}}\\\\
\footnotesize{\texttt{(  off topic  ) - there is only  episodes on the first disk of  dexter.\\ please hurry , with the 2 nd  fit blog }}.
\normalsize
\subsubsection{Sastav}
Što se udjela podataka tiče, vidljiva je razlika u odnosu na skup podataka za treniranje i skup za testiranje. Mikroblogova neutralnog polariteta ima podjednako mnogo, ali u skupu za trening ima više nego dvostruku više pozitivno označenih mikroblogova nego negativnih, dok u skupu za testiranje ima $50\%$ više mikroblogova s negativnom oznakom. Precizni podatci o broju i udjelima mikroblogova vidljivi su u tablici \ref{tablbroj}.

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|} 
\hline
                                      &pozitivni               &neutralni               &negativni            \\ 
\hline
Podatci za treniranje & 19652 (39.64\%) & 22195 (44.78\%) & 7723 (15.58\%) \\ 
\hline
Podatci za testiranje  & 2375 (19.33\%)   & 5937 (48.33\%)   & 3972 (32.33\%)  \\ 
\hline
\multicolumn{1}{l}{}  & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} 
\end{tabular}
\caption{Zastupljenost polariteta mikroblogova u podatcima}
\label{tablbroj}
\end{table}

\section{Eksperimentiranje}

Metrika korištena u obje implementacije zasniva se na prosječnom odazivu (engl.~\emph{Average recall}) svake od tri klase u koje su se mikroblogovi trebali klasificirati. Formalno definirano metrika je sljedeća: \[ \mathit{rezultat} = \frac{(R^P + R^N + R^U)}{3},\] gdje su pribrojnici brojnika redom jednaki točnosti prepoznavanja pozitivnih, negativnih i neutralnih primjera. Korištena metrika odabrana je zato što je korištena na posljednjoj godini natjecanja \emph{Semeval} i zato što se bolje ponaša kod nebalansiranog skupa podataka kakav je skup na kojemu se vrši treniranje i testiranje u ovoj implementaciji.

\subsubsection{Problem prenaučenosti i nedovoljne naučenosti}

Prilikom eksperimentiranja bitno je izbjeći dvije najčešće greške u treniranju modela, prenaučenost (engl.~\emph{overfitting}) i nedovoljnu naučenost (engl.~\emph{underfitting}). Kod prenaučenosti model pretjerano dobro klasificira primjerke iz skupa za učenje, odnosno pretjerano prilagođava funkciju predviđanja podatcima koji su mu dostupni što negativno utječe na moć generalizacije modela. Takav model imat će pretjerano visoku točnost na skupu za treniranje i znatno lošiju točnost na skupu za testiranje. S druge strane, nedovoljno treniranje modela je situacija u kojoj model nije dovoljno prilagodio funkciju predviđanja skupu ulaznih podataka i na taj je način napravio funkciju koja nedovoljno dobro generalizira primjerke iz skupa za treniranje, ali i posljedično loše klasificira i primjerke iz skupa za testiranje. Kako bi se takve greške izbjegle potrebno je nadzirati proces učenja modela. Prilikom svake epohe u treningu neuronske mreže mjeri se pogreška na podatcima za treniranje i podatcima za validaciju. Cilj je prekinuti treniranje u trenutku kada se greška na podatcima prestane smanjivati, odnosno prije nego počne rasti. Ako se trening prekine puno prije nego greška na skupu za validaciju prestane padati model će biti nedovoljno naučen. Ako se pak dopusti treniranje nakon što greška na skupu za validaciju počne rasti, model će biti prenaučen i sposobnost generaliziranja na neviđenim primjerima bit će mu smanjena. 

\begin{figure}
     \centering
     \subfloat[][Prenaučenost]{\includegraphics[width=0.45\textwidth]{overfit}\label{overfit}\hspace{7mm}}
     \subfloat[][Nedovoljna naučenost]{\includegraphics[width=0.45\textwidth]{underfit}\label{underfit}}
     \caption{Greške u treniranju modela \citep{al-masri_2019}}
     \label{greskeutreningu}
\end{figure}


\subsection{\gls{SVM}-model}
Svi eksperimenti u implementaciji metodama klasičnog strojnog učenja rade se na \gls{SVM}-modelu iz knjižnice \emph{scikit-learn}. Budući da su dostupni odvojeni podatci za treniranje i testiranje, ne vrši se križna validacija (engl.~\emph{cross-validation}), već se treniranje i testiranje odvija na zadanim podatcima. Jedini hiperparametar koji se može konfigurirati prilikom treniranja \gls{SVM}-modela s linearnom jezgrom jest parametar $c$ koji utječe na širinu maksimalne margine prilikom razdvajanja klasa u podatcima. Odabir manje vrijednosti hiperparametra $c$ utječe na povećanje širine maksimalne margine, ali kao posljedicu ostavlja dio podataka pogrešno razdijeljen. Odabirom veće vrijednosti smanjuje se širina maksimalne margine hiperravnine, ali zato manje podataka biva krivo razdijeljeno tijekom treniranja. Vrijednosti hiperparametra $c$ koje ima smisla probati dolaze iz iznimno širokog intervala, stoga se u ovoj implementaciji vrši treniranje za vrijednosti hiperparametra od $10^{-7}$ do $10^{15}$ povećavajući vrijednost parametra $10$ ili $100$ puta svakom iteracijom. Početni se model temelji na metodi vreće riječi (engl.~\emph{Bag-of-words}), odnosno na primjeni metode \emph{Tf-Idf}. Najgori mogući model nasumičnog pogađanja imao bi točnost od $33.32\%$, a početni model ima točnost od $41.22\%$ što ga čini korektnim početnim modelom. S obzirom na to da je razvoj značaka opisan u ranijim poglavljima, a jedini hiperparametar je $c$, rezultati eksperimenata sa značajkama opisani su na tablici \ref{tablesvm}, a utjecaj hiperparametra $c$ na točnost prikazan je slikom \ref{tablec}. Model postiže identičnu točnost s vrijednostima hiperparametra $10^2$ i $10^6$ koja iznosi $62.32\%$. Vrijednosti manje od $10^{-1}$ nisu prikazane na grafu, iako je proveden eksperiment i s njima, ali rezultat je znatno lošiji.

\begin{table}[]
\centering
\begin{tabular}{|l|l|} 
\hline
\small{Značajke}  &\small{Rezultat}       \\ 
\hline
\small{\emph{Tf-Idf} (\emph{Bag-of-Words})}& 41.22\% \\ 
\hline
\small{\emph{Spacy} vektori riječi}  & 60.90\%  \\ 
\hline
\small{{Spacy} vektori riječi + \emph{ANEW} + polaritet riječi}  & 61.91\%   \\ 
\hline
\small{{Spacy} vektori riječi + \emph{ANEW} + polaritet riječi + značajke prebrojavanja}  & 62.32\%   \\ 
\hline
\multicolumn{1}{l}{}  & \multicolumn{1}{l}{} 
\end{tabular}
\caption{Rezultati eksperimenata \gls{SVM}-modela}
\label{tablesvm}
\end{table}

\begin{figure}
\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel={Potencija parametra ($10^n$)},
    ylabel={Točnost[\%]},
    xmin=-1, xmax=15,
    ymin=62.2, ymax=62.4,
    xtick={ -1, 0, 1,  2, 3, 4,5, 6,7,8 ,9,10,11, 12, 13,14,15},
    ytick={62.2, 62.25, 62.3, 62.35, 62.4},
    legend pos=north west,
    ymajorgrids=true,
    grid style=dashed,
]

\addplot[
    color=red,
    ]
    coordinates {
    ( -1,62.27)( 0,62.23)( 1,62.31)(  2,62.32)( 3,62.29)( 4,62.28)(6,62.32)( 9,62.28)( 12,62.29)( 15,62.29)
    };
    
\end{axis}
\end{tikzpicture}
\caption{Utjecaj hiperparemetra $c$ na točnost SVM-modela}
\label{tablec}
\end{figure}

\subsection{\gls{LSTM}-model}

Eksperimentiranje u implementaciji koja koristi metode dubokog učenja znatno je većeg opsega nego kod klasičnog strojnog učenja. \gls{LSTM} arhitektura \gls{RNN}-modela ima velik broj parametara koji se mogu konfigurirati, pa je stoga skup mogućih postava eksperimenata prevelik da bi se u realnom vremenu isprobao. Hiperparametri koji utječu na točnost predviđanja koju utrenirana neuronska mreža ostvaruje su sljedeći: broj neurona u \gls{LSTM} sloju, veličina mini-serije (engl.~\emph{mini-batch}), broj \gls{LSTM} slojeva, odabir optimizacijske i propusne funkcije, udio neurona koji se napuštaju (engl.~\emph{dropout}), stopa učenja (engl.~\emph{learning rate}), podrezivanje gradijenta (engl.~\emph{gradient-clipping}) \citep{parameters}. Svi hiperparametri ne pridonose jednako poboljšanju točnosti, a zbog prevelikog broja kombinacija parametara, eksperimenti su provođeni u raznim etapama kako bi se što bolje precizirao skup eksperimenata koji donose najbolje rješenje. Početni model odabran kao referenca za daljnje eksperimente jednoslojna je povratna neuronska mreža s arhitekturom \gls{LSTM} koja broji $30$ neurona trenirana do najboljeg mogućeg rezultata s veličinom mini-serije od $2048$ uzoraka i bez ikakvih dodatnih hiperparametara. Zanimljivost rezultata te mreže jest to što je točnost predviđanja pozitivnih i neutralnih primjera visoka ($65\%$ i više), a točnost predviđanja negativnih primjera iznosi puno nižih $20\%$.

\subsubsection{Jednosmjerna i dvosmjerna varijanta \gls{LSTM} arhitekture}

Dodatna značajka koju je potrebno odabrati je vrsta \gls{LSTM} arhitekture. Postoji jednosmjerna (engl.~\emph{undirectional}) i dvosmjerna (engl.~\emph{bidirectional}) arhitektura. Nadogradnja na jednosmjernu arhitekturu prilično je jednostavna. U jednosmjernoj arhitekturi za izračun vrijednosti koristi se prethodni izlaz i trenutna ulazna vrijednost, što bi se moglo definirati kao \emph{pogled u prošlost}. Kod dvosmjerne arhitekture konačna vrijednost računa se kao kombinacija vrijednosti dobivene u prolazu u jednom i u drugom smjeru. U glavnini eksperimenata korištena je verzija s dvosmjernim prolazom i korištenjem iste ostvaren je i najbolji rezultat.

\subsubsection{Optimizacijska funkcija i stopa učenja}

Najkorištenija optimizacijska funkcija u \gls{LSTM} arhitekturi povratne neuronske mreže je \emph{Adam} \citep{kingma2014adam}. Temelji se na stohastičkom gradijentom spustu (engl.~\emph{Stohastic Gradient Descent}, SGD) koji predstavlja osnovni optimizacijski algoritam u dubokom učenju. Za razliku od standardnog pristupa koji unaprijed određuje kako će se stopa učenja mijenjati, \emph{Adam} koristi adaptivnu stopu učenja, odnosno korigira ju u iteracijama učenja. Upravo je po adaptivnoj stopi učenja i dobio ime (engl.~\emph{adaptive moment estimation}). U mnogim situacijama pokazao se kao dobar odabir optimizacijske funkcije, a zbog svoje brzine pogodan je za eksperimente, stoga implementacija u ovom radu koristi isključivo \emph{Adam} optimizacijsku funkciju. Formalno definirano proces osvježavanja parametara je sljedeći:
\[ m_{w}^{(t+1)} \leftarrow \beta_{1} m_{w}^{(t)}+\left(1-\beta_{1}\right) \nabla_{w} L^{(t)} \]
\[ v_{w}^{(t+1)} \leftarrow \beta_{2} v_{w}^{(t)}+\left(1-\beta_{2}\right)\left(\nabla_{w} L^{(t)}\right)^{2} \]
\[ \hat{m}_{w}=\frac{m_{w}^{(t+1)}}{1-\beta_{1}^{t+1}} \]
\[ \hat{v}_{w}=\frac{v_{w}^{(t+1)}}{1-\beta_{2}^{t+1}} \]
\[( w^{(t+1)} \leftarrow w^{(t)}-\eta \frac{\hat{m}_{w}}{\sqrt{\hat{v}_{w}}+\epsilon},  \]

\noindent gdje je $L^{t}$ funkcija gubitka, a $w^{t}$ parametri u iteraciji $t$, $\epsilon$ je mala konstanta koja sprječava dijeljenje s nulom, a $\beta _1$ i $\beta _2$ su faktori zaboravljanja gradijenta. 

\subsubsection{Broj neurona u sloju i broj \gls{LSTM} slojeva}

Dodavanjem slojeva neuronskim mrežama povećava se mogućnost apstrakcije i pronalaženja povezanosti u sadržaju. To je osobito korisno u obradi prirodnog jezika, jer jezične povezanosti ponekad su na visokoj razini apstrakcije. Kada je u pitanju primjena \gls{LSTM} arhitekture, više slojeva ni u kojem slučaju ne garantira poboljšanje modela. Nizom testiranja utvrđeno je da su dva sloja optimalna u ostvarenju rješenja zadatka kojim se ovaj rad bavi. 
Kada su u pitanju veličine slojeva, odnosno broj neurona u slojevima, najbolji je rezultat ostvaren korištenjem samo $30$ neurona, a vrlo sličan rezultat ostvaren je i s $50, 75$ i $150$ neurona, što opet pokazuje da veći broj nije nužno garancija boljeg rješenja. 

\subsubsection{Napuštanje neurona}

Osnovna zadaća tehnike napuštanja neurona (engl.~\emph{dropout}) je sprječavanje prenaučenosti modela. Princip je rada jednostavan, sa zadanom konstantom $\alpha \in [0,1)$ određuje se udio neurona koji će biti napušteni, a odabir neurona koji se napuštaju prepušten je nasumičnom odabiru. Većina najboljih rezultata ostvarena je korištenjem $\alpha = 0.5$, a jedan primjer porasta maksimalne točnosti modela povećanjem parametra $\alpha$ vidljiv je u tablici \ref{dropout}.

\begin{table}[]
\centering
\begin{tabular}{|l|l|} 
\hline
$\alpha$ &Najbolji rezultat    \\ 
\hline
0 & 62.32\% \\ 
\hline
0.25  & 63.33\%  \\ 
\hline
0.5 & \textbf{63.99\%}   \\ 
\hline
\multicolumn{1}{l}{}  & \multicolumn{1}{l}{} 
\end{tabular}
\caption{Utjecaj hiperparametra $\alpha$ na točnost \gls{LSTM}-modela}
\label{dropout}
\end{table}


\subsubsection{Veličina mini-serije}

Veličina mini-serije (engl.~\emph{mini-batch}) predstavlja broj elemenata u podskupu podataka na kojem model iterativno trenira prilagođavajući svakom iteracijom težine. Smanjivanjem veličine mini-serije značajno se usporava treniranje, ali provođenjem eksperimenata pokazalo se da su manje veličine mini-serije pogodnije za ovu vrstu problema. Iako je broj eksperimenata nedovoljan da bi se izvukao valjan zaključak, može se pretpostaviti da je optimalan raspon veličine mini-serije u intervalu između $32$ i $64$. Rezultati u tablici \ref{minibatch} ostvareni su različitim kombinacijama hiperparametara, a najbolji rezultat za svaku veličinu mini-serije zapisan je u tablici. Eksperimenti su provedeni i na većim veličinama mini-serije, ali zbog uočene tendencije da je manja veličina pogodnija za treniranje glavnina eksperimenata provedena je s veličinama iz intervala $[16,256]$.

\begin{table}
\centering
\begin{tabular}{|l|l|} 
\hline
Veličina mini-serije &Najbolji rezultat    \\ 
\hline
16 & 61.35\% \\ 
\hline
32  & \textbf{64.24}\%  \\ 
\hline
64 & 63.99\%   \\ 
\hline
128 & 63.04\%   \\ 
\hline
258 & 63.36\%   \\ 
\hline
\multicolumn{1}{l}{}  & \multicolumn{1}{l}{} 
\end{tabular}
\caption{Utjecaj veličine mini-serije na točnost \gls{LSTM}-modela}
\label{minibatch}
\end{table}

\subsubsection{Funkcija gubitka}

Važna komponenta modela je funkcija gubitka (engl.~\emph{loss function}). Standardni odabir za problem višekategorijske klasifikacije je funkcija kategoričkog gubitka unakrsne entropije (engl.~\emph{categorical cross-entropy loss}). Koristi se kada je primjer potrebno klasificirati u jedinstvenu klasu, odnosno kada je moguća samo jedna točna klasifikacija. Formula za izračun gubitka je: \[ L(l, p)=-\sum_{j=0}^{M} \sum_{i=0}^{N}\left(l_{i j} * \log \left(p_{i j}\right)\right), \]
gdje je $l$ oznaka klase, a $p$ predikcija. Izračun se provodi nad binarnim vektorom oznaka klasa koji se sastoji od jedne jedinice i $k-1$ nula, gdje je $k$ broj klasa u koje se vrši predikcija. Takvo se kodiranje naziva jedinično vruće kodiranje (engl.~\emph{one-hot encoding}). Slikovito rečeno funkcija uspoređuje vektor predviđanja s binarnim vektorom oznaka.

\subsubsection{Podrezivanje gradijenta}

Primjena podrezivanja gradijenta nije se pokazala korisna, dostatan broj eksperimenata pokazao je da gotovo nema nikakav utjecaj na točnost, a ukoliko je i postojala mala razlika, ona nije uvijek bila niti u pozitivnom niti u negativnom smjeru, stoga podrezivanju gradijenta u većini eksperimenata nije pridana pozornost.

\subsubsection{Programska implementacija}

Implementacija modela ostvarena je korištenjem knjižnice \emph{Keras} \citep{chollet2015keras} koja kao osnovu koristi knjižnicu \emph{Tensorflow} \citep{tensorflow2015-whitepaper}. U njoj su ukomponirani svi potrebni elementi korišteni prilikom izrade modela i provođenja eksperimenata. Odabir optimizacijske i propusne funkcije vrši se pozivom njenog imena, jer su sve poznatije funkcije ukomponirane u knjižnicu. Također pruža mogućnost pohrane izgrađenog modela što značajno olakšava eksperimentiranje s brojem epoha treniranja. Jedina prilagodba koja je bila nužna zbog metrike koja se koristila na natjecanju \emph{Semeval} bila je dodavanje funkcije koja računa prosječan iznos odziva po kategorijama u koje je model vršio klasifikacije.

U sklopu knjižnice \emph{Keras} postoji mogućnost automatskog uranjenog završetka treniranja (engl.~\emph{early-stopping}) oslanjajući se na željeni parametar kao što je npr. greška predviđanja na skupu podataka za validaciju. Budući da je broj epoha za sve varijacije implementacije manji od $10$, automatsko prekidanje manje je pouzdano od ručnog nadgledanja treninga, stoga metoda nije implementirana u treniranje modela.

\section{Osvrt na rezultate}

Implementacija klasičnim metodama strojnog učenja ostvarila je konačnu točnost od $62.3\%$, a implementacija metodama dubokog učenja $64.24\%$ i time ušla u 10 najboljih rezultata natjecanja \emph{Semeval}. Rezultat je zadovoljavajuć s obzirom na manji opseg eksperimenata koji su provedeni na modelu s \gls{LSTM} arhitekturom. Nekoliko je postupaka koji bi se mogli implementirati kako bi se model unaprijedio kao što je izrada vlastitih vektora riječi na temelju mikroobjava s \emph{Twittera} umjesto korištenja gotovih vektora riječi iz knjižnice kao što je \emph{Spacy}. Također bi se mogao implementirati mehanizam pozornosti (engl.~\emph{Attention mechanism}) koji je zadnjih godina zaprimio značajnu pozornost, a koji je jedan od razloga uspješnosti Googleova modela \emph{BERT}. Zbog ograničenog vremena valjano je isproban samo jedan optimizacijski algoritam (\emph{Adam}), što ostavlja mogućnost daljnjeg povećanja točnosti. Što se \gls{SVM}-modela tiče, implementirane su poprilično jednostavne značajke, pa je rezultat i viši nego bi se moglo očekivati, ali implementiranje pametnijih značajki svakako bi doprinijelo porastu točnosti. U tablici \ref{pocetakkraj} vidljiva je točnost modela, odnosno odziv po svakoj klasi u koju se radila klasifikacija. Oba početna modela najslabije su klasificirali negativne primjere. \gls{SVM}model najbolje je klasificirao pozitivne primjere, a \gls{RNN}-model sa \gls{LSTM} arhitekturom podjednako dobro je klasificirao pozitivne i neutralne primjere. U konačnom modelu došlo je do promjene u odzivu pozitivnih i negativnih primjera za manje od $10\%$ u odnosu na početni model, ali točnost odziva za negativne primjere skočila je na gotovo $300\%$ početne točnosti. U \gls{SVM} modelu značajno je narasla točnost odziva za neutralne i negativne primjere. Za negativne primjere porast je iznosio preko $100\%$, a za neutralne nešto manje od $100\%$. Prepoznavanje pozitivnih primjera također je naraslo za nešto manje od $10\%$.


\begin{table}
\centering
\begin{tabular}{|l|l|l|l|l|} 
\hline
				& verzija &negativni               &neutralni               &pozitivni            \\ 
\hline
SVM-model &  početni & 24.19\% &  33.79\% & 65.7\% \\
\hline
SVM-model & konačni & 54.26\% & 62.19\% & 70.5\% \\ 
\hline
RNN sa LSTM arhitekturom &početni & 22.96\% &66.48\%&64.22\%  \\ 
\hline
RNN sa LSTM arhitekturom &konačni & 64.23\%&59.74\%&68.74\%  \\ 
\hline
\multicolumn{1}{l}{}  & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{}  & \multicolumn{1}{l}{} 
\end{tabular}
\caption{Točnost modela po klasama }
\label{pocetakkraj}
\end{table}

\chapter{Zaključak}

Cilj je ovog rada upoznavanje s analizom sentimenta u tekstu, posebice mikroblogovima društvene mreže \emph{Twitter}. Tema je obrađena uz pomoć zadatka s natjecanja \emph{Semeval} u kojem je osnovni podzadatak određivanje polariteta mikroobjave, odnosno klasifikacija objave u pozitivnu, neutralnu ili negativnu skupinu. Najbolji model za rješavanje navedenog zadatka ostvaruje prosječan odziv u iznosu od $68.1\%$. U ovom radu rješavanju problema pristupljeno je iz dva pravca. Jedan je koristio metode klasičnog strojnog učenja, konkretno \gls{SVM}-model, a drugi metode dubokog učenja, konkretno \gls{RNN}-mrežu sa \gls{LSTM} arhitekturom. Prvim pristupom ostvaren je prosječan odziv od $62.3\%$, a drugim pristupom ostvaren je bolji rezultat od $64.24\%$. S tom točnošću model zauzima $10.$ mjesto na natjecanju \emph{Semeval 2017}. U radu je proučena izrada značajki za oba pristupa i time se pokazalo kako su pristupi izradi značajki vrlo različiti, odnosno da u slučaju primjena klasičnih metoda strojnog učenja zahtijevaju puno više pozornosti. Uspješnost modela u klasičnom pristupu počiva upravo na izradi značajki, a s druge strane, u dubokom učenju izrada značajki malen je korak u izradi modela. Većina posla koji je potrebno odraditi prilikom gradnje i treniranja modela u dubokom učenju odlazi na odabir i prilagodbe značajki modela i hiperparametre. Opseg eksperimenata koji zahtjeva kvalitetno pronalaženje hiperparametara vremenski je i računski zahtjevan posao. Rezultat modela daleko je od najboljeg koji je ostvaren na natjecanju i za njime zaostaje za nešto manje od $4\%$. Kako bi se ta razlika smanjila potrebno je unaprijediti implementaciju, a nekoliko mogućih koraka koji bi poželjno doprinijeli točnosti modela su korištenje vlastoručno izrađenih vektora koji se temelje na tekstovima mikroobjava platforme \emph{Twitter}, dodavanje mehanizma pozornosti u duboko učenje i konačno bolja obrada ulaznih podataka u pogledu korištenja pametnije tokenizacije. U ovom je radu korišten tokenizator knjižnice \emph{Spacy} koji nije posebno prilagođen za tip podataka kakav se nalazi u skupu podataka. Ugradnjom navedenih poboljšanja, a i opsežnijom provedbom eksperimenata, može se očekivati unaprjeđenje modela i povećanje njegove točnosti.
Prostora za poboljšanje ostalo je još puno, s obzirom na to da \emph{state-of-the-art} model postiže točnost od $68.1\%$ koja nije impresivna s obzirom na točnost koja se postiže na ostalim problemima iz područja analize sentimenta \citep{losirez}. \emph{Twitter} je jedna od najpopularnijih društvenih platforma današnjice, stoga je značajna i vrijednost implementacija koje se bave analizom mikroobjava s \emph{Twittera}, pa se može očekivati daljnji napredak u ovom području.

\bibliography{literatura}
\bibliographystyle{fer}
\printglossaries

\begin{sazetak}
Ovaj se rad bavi analizom sentimenta u mikroblogovima društvene platforme \emph{Twitter}. Proučavanje teme analize sentimenta ostvareno je uz pomoć četvrtoga zadatka s natjecanja \emph{Semeval} koji je bio vrlo popularan zadatak nekoliko godina u nizu u kojima se natjecanje održavalo. Napravljena je usporedba pristupa klasičnim strojnim učenjem, odnosno korištenja \gls{SVM}-modela i pristupa dubokog učenja, odnosno korištenja povratne neuronske mreže (\gls{RNN}) s arhitekturom ćelije s dugoročnom memorijom (\gls{LSTM}). S pristupom klasičnog strojnog učenja ostvarena je točnost od $62.3\%$, a pristupom dubokog učenja ostvarena je nešto bolja točnost od $64.24\%$, što implementaciju stavlja na 10. mjesto implementacija koje su bile predane u sklopu natjecanja 2017. godine.

\kljucnerijeci{strojno učenje, duboko učenje, obrada prirodnog jezika, analiza sentimenta, analiza mikroblogova, Semeval} 
\end{sazetak}

\engtitle{Machine Learning for Sentiment Analysis in Microblogs}
\begin{abstract}
The theme of this thesis is sentiment analysis on micro blogs from the social platform \emph{Twitter}. The study of sentiment analysis was done with a help of \emph{Semeval} competition task 4, very popular and attractive task for several years in a row. Comparison of traditional machine learning and deep learning techniques was made by implementing two models, one using \gls{SVM}, and the other one using \gls{RNN} with \gls{LSTM} architecture. The best result with \gls{SVM} model was $62.3\%$, and $64.24\%$ with \gls{LSTM}. Implementation would take 10th place on scoreboard of \emph{Semeval 2017} edition of the competition.

\keywords{Machine learning, Deep learning, Natural language processing, Sentiment analysis, Microblogs analysis, Semeval.}
\end{abstract}


\end{document}
